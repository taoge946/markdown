# RNN循环神经网络

RNN主要用来处理序列数据，比如说你说一句话然后预测下一个单词是什么，单词之间并不是独立的，根据预测的单词再去预测下一个单词这样循环的去预测，每一个预测之间都是有联系的。

之所以RNN被称为循环神经网络，因为一个序列的当前输出与前面的输出也有关

具体的表现形式就是网络会对前面的信息进行记忆并应用于当前输出的计算中，隐藏层之间的节点不再是无连接，而是有链接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻的隐藏层的输出

![image-20211102203613081](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211102203613081.png)

h代表RNN的网络模型，我们将模型按照时间展开，因为当我们输入的时候我们是按照时间将句子输入进模型的，比如我们想将一个句子翻译成英文，我们就得一个单词一个单词的输入再一个单词一个单词的翻译，但单词之间也有相互联系

循环神经网络其实就一个，在不停的循环如X为输入的中文单词，o为输出的英文单词，在输出英文单词的同时也保留这次翻译的状态，这个状态再配合下一个单词进行翻译，又会输出一个隐藏的状态。将它沿时间展开，V就是隐含的状态，这个状态就代表着我对前面句子的理解，因为对于前面输入的理解，有助于我们对于下一个输入的翻译，单词之间不是没有关系的，我们需要正确的了解当前的语境，即V

可以看出对每一个时刻，是由两个输入（当前时刻的输入单词以及之前的状态）和两个输出（当前时刻的翻译输出以及当前的状态）

状态输出代表我们对当前这句话整体含义的理解，即对语境的理解



一个RNN能够逐一的处理句子中的所有单词，将句子中的第一个单词传递给RNN单元，RNN单元生成输出和中间状态。该状态是序列的连续含义，由于在完成对整个序列的处理之前不会输出此状态，所以将其称为**隐藏状态（V）**从外面看不到这个状态，隐藏状态的目的是保持句子的连续含义，我们可以用最后的隐藏状态作为分类特征

对于文本情感分析任务，我们只需要判断一个句子是正面的评价还是负面的评价只考虑整体含义而可以忽略每个单元的输出，

*因为每个单词都使用了相同的RNN单元，所以大大减少了神经网络所需要的参数量，这使得我们能够处理较大规模的小批次数据*。                           （因为一直在复用一个RNN单元）

网络参数进行学习的方式是处理序列的顺序，这是==RNN的核心原则==（是按输入的顺序进行学习的，所以能够抓住语义的顺序关系）

***如***： 我们要想预测一个房子的房价，与地理位置大小等都有关系，但是这些关系和顺序无关，所以使用普通的机器学习模型就可以。    如果我们想按照年份来预测房子的价格，从10年，11年一直到21年这个时间是有顺序的，所以得使用RNN模型。再比如我们要预测明天的天气，那么最近30天的大气压等等就与顺序有关

## 简单的RNN神经元结构

一个最简单的RNN结构如下所示

![image-20211102205921744](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211102205921744.png)

hidden为隐藏层，上一时刻的隐藏层输出与当前的输入进行合并(torch.cat)

i-to-o为一个从输入到输出的linear层



torch内置了这种简单的RNN单元，可以通过以下方式导入

```python
torch.nn.RNNCell() #是对RNN单元的一个最简单的实现，带了cell就说明这只是一个单元结构
                   #这个RNNcell只给出一个状态输出，并没有当前事件的输出
```

就是只有两个输入(s<sub>t-1</sub>,i)和一个输出(s<sub>t</sub>)

接下来我们可以对于文本信息使用RNN，下面为具体的案例



## 简单文本分类于词嵌入表示

### 1.文本的数据的表示方法 与 词嵌入（embdeing）

文本是常用的序列化数据的类型之一，因为文本的顺序对于我们获取到这个文本所要表达的意思是十分重要的，从左到右读和随机跳着读所得到的信息肯定是不一样的。与图片不同，我们处理图片时就是将图片的数据直接就全扔进去了，模型在处理序列问题时，模型不是一下子获得所有的数据，而是按照顺序处理好一个再输入一个

文本数据可以看成是一个字符的序列或是词的序列，对于大多数问题，我们都将文本看做词序列

RNN及其变体能够很好的对序列化的数据建模，可以解决以下领域的问题：

- 自然语言理解
- 文献分类，情感分类
- 问答系统等

深度学习的模型本省并不能理解文本，所以要将文本转换为数值的表示形式，这个过程被称为**向量化过程**，可以用不同的方式来完成，有以下几种方法：

- 传统机器学习：tf-idf算法

- 独热编码 one-hot或者 k-hot 

  比如我们有三个单词，我们创建一个长度为3的张量，当第一个单词出现时第一个维度标志为1，其他位置标志为0

  优点：能让每一个单词都单独表示      缺点：维度非常大，创建的张量的维度必须与单词的数目一致；得自己建词库

- 散列编码

  对于当前文本使用哈希函数直接计算出一串数字。哈希函数可以看成一个加密的函数，它可以将一个任意长度的文本转换成一个定长的序列。如在网页输入密码登录时网页并不会直接保存你的密码，而是保存哈希加密值

  缺点：1.没有明确文本与文本之间的关系，如 ’北京‘ 和 ‘首都’只是两段毫无关系的数字序列。

  ​			2.因为是变成固定长度，所以有可能会出现散列冲突，单词较多时可能将两个单词计算到了同一个散列值

- 文本词嵌入(word embedding)

  在深度学习中应用最广泛的文本数值化形式，一般会将一个单词映射到一个高维的密集向量中（词向量）来代表这个单词。

  与独热编码中只有一个维度是1其他是0不同，这个是密集表示，文本词嵌入所创建的这个张量上每个维度都有数值。如

  ```html
  '机器学习'=[1,2,3]
  '深度学习'=[1,3,3]
  '李金涛'=[9,6,5]
  ```

  对于词向量，我们可以使用**余弦相似度**在计算机中来判断单词之间的距离，就是说我们可以根据这个距离来表示单词之间的相似性，如机器学习和深度学习的含义是非常相似的，所以他们之间的文本词嵌入张量之间的距离就很近

  **词嵌入**实际上是一种将各个单词再预定的向量空间中表示为实值的一类技术

  每个单词被映射成一个向量（随机初始化），并且这个向量可以通过神经网络的方式来学习和更新，学习的是单词与单词之间的相似性。

  词嵌入用密集的分布式向量来表示每个单词，好处是维度比较少，极大的减少了计算量和储存量

  词向量表示方式依赖于单词的使用习惯，这就使得具有相似的使用方式的单词具有相似的表现形式如`首都`和`北京`在向量空间内的距离就很近

  在训练时可以使用词库来训练这个网络如百度百科，谷歌等，学习过程中一般与某个神经网络的模型任务一同进行，如文档分类

  有两个比较有名的词嵌入算法：

  - Word2Vec：能够有效的从文本语料库中学到独立词嵌入的统计方法
  - GloVe:是上面的拓展，更加有效

### 使用词嵌入

我们可以自己训练一个词嵌入网络，或是使用别人训练好的词嵌入网络（类似于预训练模型）

文本数据在表示为独热编码或是词嵌入之前，需要先表示成为**token**,（每个单词表示成为一个token）将文本分解成token的过程被成为**分词**。

我们如果想要将文本数据转换为token序列，那么我们就就需要将每个token映射到向量。

在python中有很多强大的库可以用来进行分词。

实战如下：<a href='D:\python\code\pytorch深度学习\RNN循环神经网络.ipynb'>jupyter文件</a>

#### 1.去掉标点符号并全部变成小写

```python
s='Life is not easy for any of us.We must work, and above all we must believe in ourselves.We must believe that each one of us is able to do some thing well.And that we must work until we succeed.'
import string
string.punctuation#使用这个库来处理标点符号，这个库中是所有的标点符号，将句子中的标点符号全部去掉
for c in string.punctuation:
    s=s.replace(c,' ').lower() #将所有的标点符号用空格替换,并将所有英文字母变成小写
```

#### 2.进行分词

**第一种分词方式**

将每一个字符进行分词，直接用list()`list(s)`,获得了每一个字符，然后就可以使用独热编码或是词嵌入将每一个字符映射到一个张量

![image-20211103201453249](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211103201453249.png)

**第二种分词方式**

按照单词进行分割，按空格进行分词，使用`s.split()`



**第三种分词方式**

使用拓展库进行分词如使用n-gram 进行分词，可以指定分成几个部分然后进行分词，此时一个部分就不仅仅是几个单词了，而是会将附近的几个单词映射到一块

#### 3.向量化

常用的方法为one-hot独热编码和embeding词嵌入表示,此时我们按照单词分词来进行向量化

我们先找出唯一的单词再创建一个词表，表示这个单词被表示为哪个单词

```python
np.unique(s.split()) #使用np.unique来获取所有不重复的词

vocab=dict((word,index) for index ,word in enumerate(np.unique(s.split())))#创建词表
s=[vocab.get(w) for w in s.split()]#讲这句话用词表表示出来
```

#### 4.独热编码  词嵌入表示

独热编码：

这句话中一共有28个不重复的单词，所以独热编码有28维

```python
b=np.zeros((len(s),len(vocab)))#先创一个全零的张量，尺寸为实际的长度和不重复的长度，将每个位置的词的位置标志为1，其他位置还是0
for index,i in enumerate(s):
    b[index,i]=1  #比如说s的第一个单词是12，所以这个独热编码矩阵的第一行的第12列标志为1
```

词嵌入表示：

使用`torch.nn.Embdding()`

```python
em=torch.nn.Embedding(len(vocab),20)#创建一个词嵌入层，第一个参数为词库的数量（有多少不重复的单词），第二个为要映射到多长的张量上
s_em=em(torch.LongTensor(s))#将s这句话映射到向量空间上,相当于将s内的42个单词映射到长度为20的张量上
							#这个映射是可训练的，会越来越接近单词的含义，反映单词与单词之间的关系
```




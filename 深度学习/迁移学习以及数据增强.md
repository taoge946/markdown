# 迁移学习以及数据增强

## 前言

作为人类，我们有能力转移在一项任务中获得的知识，以将其用于另一项任务，任务越容易，知识的利用就越容易。比如我们会骑自行车再去学摩托车就会很容易，但是大多数机器学习和深度学习算法都旨在解决特定任务。 如果分布发生变化，这些算法将再次被重建，并且由于需要计算能力和大量时间，将很难重建和重新训练它们。

迁移学习就是关于如何使用预先训练的网络并将其应用于我们的自定义任务，以及转移从先前任务中学到的知识。迁移学习是我们采用VGG 16和ResNet之类的架构的地方，它们是许多架构和基于他们已经学到的内容进行大量超参数调整的结果，我们将该知识应用于新的任务/模型，而不是从头开始，这被称为迁移学习。

***预训练网络***是一个保存好的之前已在大型数据集或是大规模的图像分类任务上已经训练好的卷积神经网络，如果原始的数据集足够大且足够通用，那么预训练网络学到的特征空间层次结构可以作为有效的提取视觉特征的模型。

> 预训练模型的卷积部分可以看成一个特征提取部分,综合利用前面提取到的特征，利用我们自己的分类器及逆行分类。

优点就是即使当前的数据集十分的少，但是我们可以移植预训练模型，使用这个已经训练好的模型进行当前的特征的提取

![image-20210913203038661](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913203038661.png)

这些pytorch内置的预训练网络是在ImageNet这个数据集上进行训练的。

其中最常见的是VGG

## VGG

### VGG的发展过程


VGG全称是Visual Geometry Group,属于牛津大学科学工程系，其发布了一些列以VGG开头的卷积网络模型，可以应用在人脸识别、图像分类等方面，分别从VGG16~VGG19。

VGG研究卷积网络深度的初衷是想搞清楚卷积网络深度是如何影响大规模图像分类与识别的精度和准确率的，最初是VGG-16,号称非常深的卷积网络全称为(GG-Very-Deep-i6 CNN)。

VGG模型结构简单有效，前几层仅使用3×3卷积核来增加网络深度，通过max pooling (最大池化)依次减少每层的神经元数量，最后三层分别是2个有4096个神经元的全连接层和一个softmax层。

VGG在加深网络层数同时为了避免参数过多，在所有层都采用3x3的小卷积核，卷积层步长被设置为1。VGG的输入被设置为224x244大小的RGB图像，在训练集图像上对所有图像计算RGB均值，然后把图像作为输入传入VGG卷积网络，使用3x3或者1x1的filter，卷积步长被固定1。

vGG全连接层有3层，根据卷积层+全连接层总数目的不同可以从VGG11 ~ VGG19，最少的VGG11有8个卷积层与3个全连接层，最多的VGG19有16个卷积层+3个全连接层，此外VGG网络并不是在每个卷积层后面跟上一个池化层，还是总数5个池化层，分布在不同的卷积层之下.

![image-20210913203514362](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913203514362.png)

### VGG的缺点

1.网络架构weight数量相当大，很消耗磁盘空间。

⒉训练非常慢

由于其全连接节点的数量较多，再加上网络比较深，VGG16有533MB+，VGG19有574MB。这使得部署VGG比较耗时。



VGG在加深CNN网络深度方面首先做出了贡献，但是VGG也有自身的局限性，不能无限制的加深网络，在网络加深到一定层数之后就会出现训练效果褪化、梯度消逝或者梯度爆炸等问题，总的来说VGG在刚提出的时候也是风靡一时，在
lmageNet竞赛数据集上都取得了不错的效果



### VGG的使用

```python
models=torchvision.models.vgg16(pretrained=True)#使用预训练好的VGG模型(如果本地没有会下载)
```

下载好之后我们就可以直接使用这个模型了,不过我们需要根据我们的需要改变最后一层的参数数量以及分类器

![image-20210913204608175](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913204608175.png)

我们使用训练好的卷积基部分来提取特征，但我们需要重新训练它的分类器（全连接层即linear层）所以我们需要将预训练好的模型的卷积基保留下来，将分类器替换掉

可以从下图的VGG模型结构看出，VGG由两个部分组成，即卷积部分features和分类器部分classifier（全连接层），是针对ImageNet 1000分类问题的

卷积部分的左后使用了avgpool为全局平均池化，可以代替view的方法

![image-20210913204645591](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913204645591.png)

![image-20210913204650903](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913204650903.png)

可以从上图看出，VGG模型的最后使用的是classifier的分类器。

我们要保留下来卷积的部分，让它不再进行梯度更新

```python
for p in models.features.parameters():
    p.requires_grad=False
```

对于分类器就让他输出为自己想要的类别就可以了，比如天气问题就让最后的输出改为4

![image-20210913204756506](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913204756506.png)

我们只需要改classifier分类器的最后一个线性层，所以就取最后一个

![image-20210913204807562](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913204807562.png)

```python
models.classifier[-1].out_features=4 #将最后一层的输出改为4
```

然后再接着设定损失函数和优化函数

```python
optim=torch.optim.Adam(models.classifier.parameters(),lr=0.001) #只对最后一个分类器的参数进行优化
loss_fn=nn.CrossEntropyLoss()
```



用fit进行训练就可以了，可以看出训练一开始的正确率就非常的高。



## Resnet预训练模型

和VGG模型一样，需要从torchvision先加载模型

```python
model=torchvision.models.resnet18(pretrained=True)
```

可以看一下这个模型

![image-20210914205647604](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210914205647604.png)

和VGG不同的是，RESNET全局都是con，pool最后在线性分类前进行了一次avgpool全局池化

最后也只是用了一个linear层

首先要让预训练参数不计算梯度

```python
for p in model.parameters():
    p.requires_grad=False
```

然后就是替换最后一层线性层。这样还有一个好处就是最后替换的层是可训练的，其他层设置的是不可训练。

```python
in_f=model.fc.in_features #取出原模型的线性层参数中的输入参数
model.fc=nn.Linear(in_f,4)#根据原模型的输入参数进行线性层的替换，输出为4分类

device=torch.device("cuda:0"if torch.cuda.is_available() else "cpu")
if torch.cuda.is_available():
    model.to('cuda')           #将模型放在显卡上，然后再x.to(device),y,to(device)了，将参数也放在了显卡上运算

opt=torch.optim.Adam(model.fc.parameters(),lr=0.001) #只对最后一个线性层的参数进行优化
loss_fn=nn.CrossEntropyLoss()
```

> 使用RESNET预训练模型时必须在fit函数中声明是训练部分和测试部分，因为RESNET中有BN层

> 这种预训练模型先查看模型再对最后一层线性层或是分类器进行修改到自己需要的参数就可以直接使用了

### 预训练模型卷积层微调

预训练模型的卷积层的参数是对ImageNet的图片敏感的，对新图片可能不是特别的契合，我们可以也对卷积部分进行训练。

我们需要先冻结卷积部分的参数，将分类器部分训练好了之后再对卷积部分进行微调，如果有没有这样的话，刚开始的训练误差很大，微调之前这些卷积层学到的表示会被破坏掉。

要先训练好分类器，再将卷积部分放开，和分类器再一起训练

***微调步骤***

1. 在预训练卷积基上添加自定义层
2. 冻结卷积基所有层
3. 训练添加的分类层
4. 解冻卷积基的一部分层（一般是解冻靠近输出的卷积基，也可以全部解冻）
5. 联合训练解冻的卷积层和添加的自定义层

```python
#使用resnet101，前面的处理步骤同上
model=torchvision.models.resnet101(pretrained=True)
for p in model.parameters():
    p.requires_grad=False
#微调前
in_f=model.fc.in_features #取出原模型的线性层参数中的输入参数
model.fc=nn.Linear(in_f,4)#根据原模型的输入参数进行线性层的替换，输出为4分类

from torch.optim import lr_scheduler
opt=torch.optim.Adam(model.fc.parameters(),lr=0.001)
exp_lr_scheduler=lr_scheduler.StepLR(opt,step_size=7,gamma=0.9)
loss_fn=nn.CrossEntropyLoss()  #此时进行训练正确率能到94%

#微调
for parm in model.parameters():
    parm.requires_grad=True    #打开所有层的梯度   注意，一定要在线性层训练完成之后才能这么微调
    
opt=torch.optim.Adam(model.parameters(),lr=0.0001)#此时优化全部参数而不仅仅只是最后的线性层参数，
                                                  #微调时lr要尽量的设置的小一些
#然后进行训练能得到更高的正确率
```

### 模型权重保存(保存模型)

首先就是要保存模型的可训练参数`model.parameters()`

可以使用模型的一个方法`model.state_dict() #返回一个字典，内容为模型的所有可训练参数`（一定要加括号）

![image-20210916102233647](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210916102233647.png)

这个字典就保存了每一层的权重和偏置,将这个字典使用torch中的函数`torch.save()`将其保存到本地

```python
PATH='my_model.pth' #将模型权重保存再本目录下，一般权重文件都定义为 .pth/ .pt
torch.save(model.state_dict(),PATH)  #使用torch中的函数保存参数   #一定要加括号
```

这样仅仅是保存的模型的权重,模型本身的结构并没有被保存,使用时还需要我们自己去定义,所以我们初始化一个新的类,此时这个新初始化的模型的参数都是随机的,我们需要将保存的权重加载到这个新的类中

```python
#如果是自己创建的对象模型可以直接接着创建新的模型
new_model=Net()
#如果使用的是预训练模型则需要再导入一次。
new_model=torchvision.models.resnet101(pretrained=True)

#然后就可以往新模型中导入训练好的参数了
new_model.load_state_dict(torch.load(PATH))#将权重参数加载进来,这样就可以使用训练好的权重了
#然后别忘了把模型放在cuda上
new_model.to('cuda')
```

我们还可以使训练函数保存***最优参数***

```python
best_acc=0

for epoch in range(epochs):
    epoch_loss,epoch_acc,epoch_text_loss,epoch_text_acc =fit(epoch,model,train_dl,text_dl)
    
    if epoch_text_acc > best_acc:
        best_model_wts=copy.deepcopy(model.state_dict()) #达到目前最高的正确率时复制此时的模型权重
        best_acc=epoch_text_acc
    
    train_loss.append(epoch_loss)
    train_acc.append(epoch_acc)
    text_loss.append(epoch_text_loss)
    text_acc.append(epoch_text_acc)
    
model.load_state_dict(best_model_wts)#在整个模型训练完成后给模型最优情况的权重值，在使用这个模型之前要先设置成																					model.eval()模式
```

> 使用这种方法保存最优参数后使用时要重新定义一个模型再将参数导入

### 提取预训练模型的卷积基

```python
resnet=torchvision.models.resnet101(pretrained=True)

list(resnet.children()) #会返回一个所有层的生成器，用list将其返回,可以从下图看出，一共有十块
list(resnet.children())[:-1] #可以像这样取出除了最后一个模块linear层之外的所有层，将其添加到我们的模型当中

conv_base=nn.Sequential(*list(resnet.children())[:-1])#将除了线性层之外的所有层放入我们的卷积基模型中，使用‘*’将其解包
```

![image-20210922165224217](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210922165224217.png)

然后使用卷积基来创建自己的模型

```python
in_size=resnet.fc.in_features#通过这个获取resnet全连接层的输入参数量

class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()#继承父类的方法
        self.conv_base=nn.Sequential((*list(resnet.children())[:-1]))#初始化卷积部分，使用之前迁移学习的卷积基
        self.fc1=nn.Linear(in_size,1)#
        self.fc2=nn.Linear(in_size,1)
        self.fc3=nn.Linear(in_size,1)
        self.fc4=nn.Linear(in_size,1)#因为输出了四个坐标值（xmin,ymin,xmax,ymax）所以设置了4个linear层
    def forward(self,x):
        x=self.conv_base(x)
        x = x.view(x.size(0), -1)
        x1=self.fc1(x)
        x2=self.fc1(x)
        x3=self.fc1(x)
        x4=self.fc1(x)
        return x1,x2,x3,x4

```



---



## 数据增强

使用VGG发现还是有一点过拟合，我们可以使用**数据增强**，人为的去扩增数据，但实际的训练图片并没有变化。

我们随机的对图像做一些调整，如改变对比度，改变亮度，做一些翻转或缩放

 

torchvision是专门设计来为计算机视觉服务的，其中最重要的有三个，即`torchvision.datasets`为内置的数据（如minst手写数据集），`torchvision.models`为预训练模型，第三部分就是`torchvision.transforms`提供了转换数据的方法，或是说数据增强的方法

```python
from torchvision import transforms

train_transform=transforms.Compose([
    transforms.Resize((244,244)), #先裁剪成一个大一点的图片再随机裁剪成192x192的
    transforms.RandomCrop(192),#随机裁剪成192的
    transforms.RandomHorizontalFlip(),#加一个随机的左右翻转
    transforms.RandomRotation(0.2),#随机旋转20度   
    transforms.ColorJitter(brightness=0.5),#随机调整亮度，0.5是超参数，需要自己去更改
    transforms.ColorJitter(contrast=0.5),#随机调整对比度
    transforms.ColorJitter(saturation=0.5),#随机调整饱和度
    transforms.ColorJitter(hue=0.5),#随机调整颜色
    transforms.RandomGrayscale(p=0.5),#随机灰度化
    transforms.ToTensor(),#规范到0到1之间 ,将维度提到最前面，将图片转化为tensor格式，必须加这个变形 。必须放在最后Normalize之前，否则会报错
    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]) #标准化   从0~1转化成-1~1  
                                #两个参数分别是均值和方差，如果事先了解数据集的可以直接填上，不了解就写个大概
                                #当模型出现过拟合时，用来降低模型的复杂度
])
#可以在使用的时候根据需要选取适当的变形方法。
#每条之间要记得加逗号分隔开。

#对于测试数据集就没有必要进行这么多变形了
test_transform=transforms.Compose([
    transforms.Resize((96,96)), #把图片大小统一成96*96   要加两个括号！！！一个是函数的括号，一个是参数的括号
    transforms.ToTensor(),#规范到0到1之间
    transforms.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5]) #标准化   从0~1转化成-1~1
                                #两个参数分别是均值和方差，如果事先了解数据集的可以直接填上，不了解就写个大概
])

#使用ImageFloader的方法引入训练文件夹（需提前自己分好类放入不同的文件夹中）
train_ds=torchvision.datasets.ImageFolder(
    train_dir,                      #两个参数，一个是目录地址，一个是变形方法
    transform=train_transform  

)
text_ds=torchvision.datasets.ImageFolder(
    text_dir,                      #两个参数，一个是目录地址，一个是变形方法
    transform=test_transform  

)
```

> *ToTensor必须放在最后Normalize之前，否则会报错*

创建好transform方法后也可以直接用在图片上，不过在使用前因为ToTenser会改变图像的维度次序，要改回来而且使用plt绘图需要先变成numpy数据类型

```python
img=(img.permute(1,2,0).numpy() )
img=transform(img)
```



---



## 自动学习速率衰减

Pytorch内置了学习速率衰减的框架，在设置好优化方法后（opt），我们可以在训练过程中人为的改变学习速率

### 1.根据步数进行衰减(最常用)

```python
from torch.optim import lr_scheduler #内置的自动学习速率下降库

optim=torch.optim.Adam(model.fc.parameters(),lr=0.001)
exp_lr_scheduler=lr_scheduler.StepLR(opt,step_size=5,gamma=0.9)#第一个参数为优化器，第二个为每5步衰减一次，第三个为衰减系数   ，相当于每5步学习速率乘于0.9
```

在使用时需要在fit函数中添加一个.step()方法

<img src="C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210913211534673.png" alt="image-20210913211534673" style="zoom:150%;" />

### 2.里程碑衰减

```python
from torch.optim import lr_scheduler #内置的自动学习速率下降库
exp_lr_scheduler=lr_scheduler.MultiStepLR(opt,milestones=[20,50,80,120,150],gamma=0.7)# 第二个参数为里程碑，是一个列表，在第多少个epoch进行衰减
```

可以在固定的epoch数进行衰减，同样也是放在fit里的相同位置。

### 3.一直衰减

在每一个epoch上都进行衰减，直到最后一个epoch

```python
exp_lr_scheduler=lr_scheduler.ExponentialLR(opt,gamma=0.9) #每一个epoch都进行衰减直到最后
```


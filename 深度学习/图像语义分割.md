# 图像语义分割

### 含义

它是指像素级地识别图像,即标注出图像中每个像素所属的对象类别。如下图，对一张图片中的所有像素进行标记，背景的像素标记为蓝色，边缘标记为红色，猫标记为黄色。

![image-20210906090327810](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210906090327810.png)

语义分割对图像中每一个像素点进行分类确定每个点的类别(如属于背景、边缘或身体等)

- 与图像实例分割相区分，语义分割没有分离同一类的实例，我们关心的只是每个像素点的类别，如果输入图像中有两个相同类别的对象，则语义分割本身不将他们区分为单独的对象。我们不区分是哪一只羊，只是将所有的羊进行划分。

  ![image-20210906090651262](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210906090651262.png)

### 应用场景

1.自动驾驶汽车:我们需要为汽车增加必要的感知，以了解他们所处的环境，以便自动驾驶的汽车可以安全行驶;分辨出前面是地面是树或是人
2.医学图像诊断:机器可以增强放射医生进行的分析，大大减少了运行诊断测试所需的时间;
3.无人机着陆点判断，看当前无人机下面是水还是山地还是平地

### 图像语义分割实质:

实际上是一种分类问题，一般是将一张RGB图（h* w* 3）或者灰度图(h* w * 1)作为输入，输出是分割图（h* w*1），其中每一个像素包含了其类别的标签

![image-20210906091254658](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210906091254658.png)

![image-20210906091335823](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210906091335823.png)

### 在图像分割领域比较成功的算法

#### FCN

- Long等人提出的**全卷积网络（FCN）**

  **FCN**将分类网络转换成用于分割任务的网络结构，并证明了在分割问题上，可以实现端到端的网络训练。
  FCN成为了深度学习解决分割问题的奠基石。

- 使用普通的分类网络由于最后全连接层的存在，丢失了输入的空间信息，所以没有办法直接用于解决诸如分割等稠密估计的问题

  所以FCN网络中用卷积层和池化层替代了分类网络中的全连接层，从而使网络结构可以适应像素级的稠密估计任务。

  

  ![image-20210906092002325](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210906092002325.png)

## Unet

目前语义分割应用最广的模型，能从更少的训练图像中进行学习

![image-20210906092207131](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210906092207131.png)

- 网咯结构：非常简单，前半部分作用是特征 提取，后半部分是上采样。这样的结构在一些文献中也被叫做编码器-解码器结构

  如上图所示，这种网络结构类似于大写的英文字母U，所以叫U-net

  - 从输入开始看，经过两个3X3的卷积，因为没有设置填充所以每次卷积长宽减2
  
  - 然后下采样，两层卷积，下采样两层卷积，，，
  
  - 进行上采样（如反卷积），合并特征，再上采样合并。。。
  
    这个网络结构有个很显著的特征就是会结合前面提取的特征，进行合并，使得图像变厚
  
  - 最后使用了一个三层的卷积，两层3x3的一层1x1的。
  
    

### 图像语义分割的结构(输入输出)

网络的输入可以是任意尺寸的彩色图像,输出与输入尺寸相同,输出通道数为n(目标类别数)+1(背景)

特点是使用全卷积,最后一层不再使用线性层,而是替换成卷积层,其目一方面是保留了图像的空间结构,另一方面是允许输入任意大小的图片

#### 下采样

以前使用过的池化就是下采样如最大池化(也叫全局池化),平均池化,

或者增大卷积层的跨度,也会使图像减少

#### 上采样

图像在模型中处理时会越变越小,为了得到原图大小的稠密像素预测,我们就需要进行上采样

与上面的三种下采样相对应,使用三种方法来扩大图片

- 插值法:如在每两个像素之间插入这两个像素的均值的像素

- 反池化:如反最大池化就是一个像素为该值其他像素为0,反平均池化就是将所有像素位置填上这个像素值

  9 ->9 0       5-> 5 5

  ​	   0 0             5 5

- **反卷积**(转置卷积):最长用的上采样方法,本质上是通过训练来放大图像.

  ​							  通过训练获得反卷积核,让卷积后的结果通过反卷积核放大回去.

  ![image-20210930121951931](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210930121951931.png)

### IOU评价指标

​	loU也可以叫做交并比，是一种测量在特定数据集中检测相应物体准确度的一个标准。
​	loU是一个简单的测量标准，只要是在输出中得出一个预测范围(bounding boxex)的任务都可以用loU来进行测量,比如目标检测、语义分割等等.

​	IOU表示产生的候选框(candidate bound)与原标记框(groundtruth bound)的交叠率或者说重叠度，也就是它们的交集与并集的比值。相关度越高该值。

最理想情况是完全重叠，即比值为1。

**计算公式**为实际的目标位置与预测的目标位置的交集除以他们之间的并集.如果完全重合就说明完全正确,IOU的值为1

## 代码实现

数据集采用的是香港中文大学提供的数据集，这个数据集是创建人物的蒙版,通过语义分割的方式将其分割

![image-20210930195331621](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210930195331621.png)

在这个简单的语义分割项目中我们需要将图片的像素分成两类，一类是人，一类是背景。所以他是一个二分类问题。

我们先读取一张图片看看

```python
plt.figure(figsize=(12,8))
plt.subplot(1,2,1)
pil_img=Image.open(r'D:/数据集/HKdataset/HKdataset/testing/00001.png')
np_img=np.array(pil_img)
plt.imshow(np_img)
plt.subplot(1,2,2)
pil_img=Image.open(r'D:/数据集/HKdataset/HKdataset/testing/00001_matte.png')
np_img=np.array(pil_img)
plt.imshow(np_img)
```

![image-20210930212052847](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210930212052847.png)

看一下蒙版是不是只有两个值，是不是需要二分类

```python
np.unique(np_img)#看这个ndarray中包含哪些值
```

但是结果却是0到255都有，不符合我们的预期，我们需要就只有两种像素值如背景为0人物为1，所以我们让他像素大于0的就直接给1

```python
np_img_2[np_img_2>0]=1#将所有不为0的像素值设成1方便进行二分类。
```

但这样会损失边缘信息，原本的蒙版图像边缘十分的平滑但现在就十分的粗糙。不过这不是医学图像，不太要求边缘所以也可以



继续观察这个数据集，发现这个数据集中所有图片的大小都是800*600的，可以直接输入进模型，也可以resize成256x256的节约显存。

接下来我们创建dataset输入

首先通过glob获取所有图片的路径，然后再将图片和蒙版分开

```pytho
all_pics=glob.glob(r'D:\数据集\HKdataset\training\*.png')
images=[p for p in all_pics if 'matte' not in p]#使用中括号将原图和蒙版分开
anno=[p for p in all_pics if 'matte'  in p]
```

然后将其乱序,并将测试图片也取出来，测试图片不需要乱序

```python
index=np.random.permutation(len(images))
images=np.array(images)[index]
anno=np.array(anno)[index]

all_test_pics=glob.glob(r'D:\数据集\HKdataset\testing\*.png')
test_images=[p for p in all_test_pics if 'matte' not in p]
test_anno=[p for p in all_test_pics if 'matte' in p]
```

再定义变形方法，定义数据集子类

```python
transform=transforms.Compose([
    transforms.Resize((256,256)),
    transforms.ToTensor()
])

class HK(data.Dataset):
    def __init__(self,imgs_path,anno_path):
        self.img_path=imgs_path
        self.anno_path=anno_path
    def __getitem__(self, index):
        img=self.img_path[index]
        anno=self.anno_path[index]
        
        pil_img=Image.open(img)
        img_tenser=transform(pil_img)

        anno_img=Image.open(anno)
        anno_tenser=transform(anno_img)
        anno_tenser[anno_tenser>0]=1#将蒙版图像转换为二分类图像只有0和1
        anno_tenser=torch.squeeze(anno_tenser).type(torch.long)#现在的蒙版图像是256x256x1的，
                                    #用squeeze的方法将1的那个维度去掉,
                                    # 并转换为long类型输入模型因为必须是整数
        return img_tenser,anno_tenser
    def __len__(self):
        return len(self.img_path)
```

然后就可以根据这个子类创建ds集了,取出一个批次的数据看是不是满足要求

```python
train_ds=HK(images,anno)
test_ds=HK(test_images,test_anno)
train_dl=data.DataLoader(train_ds,batch_size=8,shuffle=True)
test_dl=data.DataLoader(test_ds,batch_size=8)

img_batch,anno_batch=next(iter(train_dl))

plt.figure(figsize=(12,8))
plt.subplot(1,2,1)
plt.imshow(img_batch[0].permute(1,2,0).numpy())
plt.subplot(1,2,2)
plt.imshow(anno_batch[0].numpy())#本来就是二维的图像了所以不用转换维度次序
```

![image-20211001113141620](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211001113141620.png)

接下来就要创建u-net模型，先观察其结构

![image-20211001113559381](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211001113559381.png)

可以发现其有大量重复的操作，u-net模型经历了多次 两次卷积之后下采样，两次卷积后上采样。我们可以将重复的动作封装起来，变成上采样模块和下采样模块，这样我们就可以将整个模型的结构进行==分块处理==：将重复的运算全部编写到一个小模块里。

再torch中我们经常使用自定义模型继承于nn.Moudel,我们经常编写一些小的自定义模型来实现卷积卷积下采样，最后再使用这些小的模型组装起来，组装成大的模型。

- 编写下采样子模型,加了一个判断，首先判断是否要做下采样（输入层就不需要做下采样）

```python
class Downsample(nn.Module):
    def __init__(self,in_channel,out_channel):
        super(Downsample,self).__init__()#继承父类的属性
        self.conv_relu=nn.Sequential(
            nn.Conv2d(in_channel,out_channel,kernel_size=3,padding=1),
                        #padding为1的话就是周围填充一圈，使卷积后图像的大小不变
            nn.ReLU(),
            nn.Conv2d(out_channel,out_channel,kernel_size=3,padding=1),
            nn.ReLU(),
        )
        self.pool=nn.MaxPool2d(kernel_size=2)#定义一个2x2的最大池化
    def forward(self,x,is_pool=True):
                #设置一个判断，判断是否要做下采样，如输入时就不需要进行下采样默认为要做
        if is_pool:
            x=self.pool(x)
        x=self.conv_relu(x)
        return(x)

```

- 编写上采样子模型

```python
class Upsample(nn.Module):
    def __init__(self,channel):#也可一根据规律来写通道数，发现输出通道数一直是输入的一半，所以定义一个通道参数也可以
        super(Upsample,self).__init__()#继承父类的属性
        self.conv_relu=nn.Sequential(
            nn.Conv2d(2*channel,channel,kernel_size=3,padding=1),
                        #padding为1的话就是周围填充一圈，使卷积后图像的大小不变
            nn.ReLU(inplace=True),#设置这个就是直接对数据进行relu不保存中间数据，这样更快不占内存但不推荐
            nn.Conv2d(channel,channel,kernel_size=3,padding=1),
            nn.ReLU(inplace=True),
        )
        self.upconv=nn.Sequential(
            nn.ConvTranspose2d(channel,channel//2,kernel_size=3,stride=2,output_padding=1)
                            #pytorch内置的反卷积层，其参数与卷积的参数几乎一摸一样，但是有些不同
                            #反卷积需要跨度，没有跨度是实现不了图像的放大的，跨度起到放大的作用,为2表示放大两倍
                            #反卷积的padding指的是输入的起始位置，为2代表从第二个像素开始做反卷积
            				#必须要加padding否则图像会增大2（32->34）,就不是原本大小了，就不能合并了
                            #outkput_padding反卷积之后在外面进行填充，类似于卷积的padding,边缘填充1来使输出图像与输入图像大小相同
            
            nn.ReLU(inplace=True)#池化不需要激活，反卷积需要激活
        )
    def forward(self,x):
        x=self.conv_relu(x)
        x=self.upconv(x)
        return x
```

下面就可以编写U-net的模型了，通过观察模型可以发现即使定义了下采样和上采样模型，我们仍需要再定义一个上采样层以及一个1x1的卷积层，具体代码如下：

```python
class Unet_model(nn.Module):
    def __init__(self):
        super(Unet_model,self).__init__()
        self.down1=Downsample(3,64)#再初始化时不需要写ispool参数，在前向传播的时候再写就行
        self.down2=Downsample(64,128)
        self.down3=Downsample(128,256)
        self.down4=Downsample(256,512)
        self.down5=Downsample(512,1024)

        self.up=nn.Sequential(
           nn.ConvTranspose2d(1024,512,kernel_size=3,stride=2,output_padding=1),
           nn.ReLU()
        )

        self.up1=Upsample(512)
        self.up2=Upsample(256)
        self.up3=Upsample(128)

        self.conv_2=Downsample(128,64)#使用下采样加判断来不进行池化仅仅当作卷积来使用

        self.last=nn.Conv2d(64,2,kernel_size=1)#因为是二分类，所以输出维度是2
    def forward(self,x):

       x1=self.down1(x,is_pool=False) #第一层对输入图片只进行卷积不进行下采样
       x2=self.down2(x1)#使用不同的变量名称因为我们要将每一层的输出保留下来上采样时进行合并
       x3=self.down3(x2)
       x4=self.down4(x3)
       x5=self.down5(x4)

       x5=self.up(x5) #这是最底层，不需要保留输出了,以后的上采样也不需要合并了

       x5=torch.cat([x4,x5],dim=1)#合并不是简单的相加，而是维度进行的合并，使用torch中的cat方法进行维度的合并
                        #在pytorch中，数据集的四个参数为为（batch，channel，w,h）所以是选择第一个维度进行合并
       x5=self.up1(x5)

       x5=torch.cat([x3,x5],dim=1)
       x5=self.up2(x5)

       x5=torch.cat([x2,x5],dim=1)
       x5=self.up3(x5)

       x5=torch.cat([x1,x5],dim=1)

       x5=self.conv_2(x5,is_pool=False)#最后也是进行两次卷积不进行池化

       x5=self.last(x5)#现在就是每一个像素点输出一个像素为2的向量

       return x5
```

模型最核心的地方就是使用cat对之前的输出的特征层进行合并，可以重复利用中间特征

因为输出是一个长度为2的张量，所以损失函数使用`nn.CrossEntropyLoss()`当多分类问题来处理

```python
loss_fn=nn.CrossEntropyLoss()#当多分类问题来处理（本质是二分类问题，输出的是一个长度为2的向量，是两个概率）
from torch.optim import lr_scheduler
optim=torch.optim.Adam(model.parameters(),lr=0.001)
exp_lr_scheduler=lr_scheduler.StepLR(optim,step_size=7,gamma=0.1)
```

然后就可以进行训练了，训练20个epoch后结果就差不多,训练与之前唯一的不同的地方在于total需要最后×256*256，因为是对这张图像的所有像素进行正确率判断的。

然后就是进行验证和测试，首先在test数据集上进行预测，先预测三张图片，显示出原图和实际蒙版以及预测蒙版看效果

> 如果使用了保存最优权重那个训练方法需要新建模型重新导入最优参数

```python
new_model=Unet_model()
new_model.load_state_dict(torch.load(PATH))

num=3
img_batch,mask_bath=next(iter(test_dl))
pred_batch=new_model(img_batch)

plt.figure(figsize=(10,10))
for i in range(num):
    plt.subplot(3,3,i*num+1)#绘制原图片
    plt.imshow(img_batch[i].permute(1,2,0).cpu().numpy())#原来是在显卡上运行的，现在将它放在cpu上

    plt.subplot(3,3,i*num+2)#绘制实际蒙版
    plt.imshow(mask_bath[i].cpu().numpy())#mask没有维度所以只将其放在cpu上就可以了

    plt.subplot(3,3,i*num+3)#绘制预测蒙版
    plt.imshow(torch.argmax(pred_batch[i].permute(1,2,0),axis=-1).detach().numpy())
                #因为是二分类切将分类的维度信息用permute放到了最后所以使用argmax获取最后一个维度的分类结果
                #用dim=-1或者axis=-1是一样的
```

![image-20211002142700653](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211002142700653.png)

效果还可以

如果我们想将这个模型应用一下，从网上照一张单张的图片进行语义分割的话，需要给图像再添一个维度当成一个batch_size只有1的一个batch才能输入进模型

```python
path='D:/python/picture/tao.jpg'
pil_img=Image.open(path)
img_tensor=transform(pil_img)

img_tensor_batch=torch.unsqueeze(img_tensor,0)
#图片输入模型是一个batch一个batch输入进模型的，相当于是4维输入进模型，
# 所以我们使用torch.unsqueeze()方法添加一个1维度在这张图片的开头
#squeeze是去掉一个内容为 1 的维度，unsqueeze是添加一个内容为 1 的维度
```

![image-20211002143650855](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211002143650855.png)

这样就可以输入进模型进行预测了,直接使用第一个`pred[0]`或是将 1 的维度给去掉`pred=torch.squeeze(pred,0)`

```python
plt.figure(figsize=(8,8))
plt.subplot(1,2,1)
plt.imshow(transform(pil_img).permute(1,2,0).numpy())
plt.subplot(1,2,2)
plt.imshow(torch.argmax(pred.permute(1,2,0),dim=-1).detach().numpy()) #或不压缩就是pred[0]一样的
```

![image-20211002145540341](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211002145540341.png)

![image-20211002150122020](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211002150122020.png)

效果还行

## LinkNet

这是resnet在图像分割领域的应用

像素级的图像语义分割不仅需要精准还需要高效，以便应用到实时应用中，如自动驾驶汽车		

这个网络本身是为了加快语义分割的速度的

==LinkNet兼顾了精度和速度==

其网络结构如下，其结构借鉴了自编码器，第一列是编码器，有四个编码层，第二列是解码器，有4个解码模块，最下面两层是输入层和输出层（full_conv是反卷积的意思）

![image-20211008141426849](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211008141426849.png)

在输入部分 /2表示对图像进行一次缩放，变成原来的二分之一

所以在输出部分进行两次放大，N为实际需要需要多少分类

> linknet设计的另一个精髓是对前面提取的特征的连接，将前面层的连接直接加到了后面的层，避免了很多层之间特征的丢失

####  **Encode Block**

![image-20211008155818893](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211008155818893.png)

Encode block是一个残差模块，通过残差模块可以有效的减少参数量，m,n是输入输出的层数

与resnet不同的是它对图像进行了一次缩放，所以在cat的时候要再应用一次卷积，将原图像也进行一次缩放

#### Decode Block

![image-20211008160238405](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211008160238405.png)

包含2个卷积层加一个反卷积层，反卷积层对图像进行放大



#### 代码实现

通过观察结构，我们可以先搭建Encode模块和Decode模块再重复调用



- 看Encode模块内部，是由四个卷积模块构成的，我们可以将卷积模块写出来（conv,bn,relu）

- 看Decode内部，除了卷积模块之外还需要编写一个反卷积模块（反卷积,bn,relu）
- 输入部分：卷积模块加一个最大池化
- 输出部分：两个反卷积模块，一个卷积模块

所以我们需要编写卷积模块和反卷积模块，使用这俩模块再去构成Encode模块和Decode模块以及输入和输出

##### 编写卷积模块

卷积模块是卷积+BN+激活

```python
class conv_block(nn.Module):
    def __init__(self,in_channel,out_channel,k_size=3,stride=1,pad=1):#因为在所有使用卷积时会使用到不同大小的卷积核（大部分为3），
                                                #有些卷积层还要进行缩放所以再定义一个stride参数（大部分为1）
                                                #使3*3的卷积后的图像大小不变，所以加一个pad参数，默认为1
        super().__init__()
        self.conv_bn_relu=nn.Sequential(
            nn.Conv2d(in_channel,out_channel,kernel_size=k_size,stride=stride,padding=pad),
            nn.BatchNorm2d(out_channel),
            nn.ReLU(inplace=True)
        )
    def forward(self,x):
        x=self.conv_bn_relu(x)
        return x
```



##### 编写反卷积模块

反卷积模块是反卷积+BN+激活

- 注意观察到输出的反卷积层不需要进行BN和激活，所以我们在forward加一个参数来控制是否要添加BN层和激活层
- 反卷积和卷积相比有两个pad参数:output_padding和padding
  - output_padding：就是对输出图像的外层进行填充
  - padding：是反卷积的起始位置，如一个3*3的反卷积，padding为1代表从第二个位置开始进行反卷积

所以此时不能再用nn.Sequential进行模型的创建了

```python
class decode_conv_block(nn.Module):
    def __init__(self,in_channel,out_channel,k_size=3,stride=2,pad=1,output_padding=1):
                        #添加一个参数output_padding默认为1使图像大小和输入大小相同
                        #stride为2的话就是将图像放大一倍
        super().__init__()
        self.deconv=nn.ConvTranspose2d(in_channel,out_channel,
                                        kernel_size=k_size,
                                        stride=stride,
                                        padding=pad,
                                        output_padding=output_padding)
        self.bn=nn.BatchNorm2d(out_channel)
    def forward(self,x,is_act=True):#给一个参数来判断是否需要激活,默认需要激活
        x=self.deconv(x)
        if is_act:
            x=torch.ReLu(self.bn(x))
        return x
```





##### 构建Encode block模块

由4个卷积模块组成。观察这个模块发现第一个卷积层进行了一次缩放，stride=2,原图的高和宽都变成了原来的二分之一，所以最后的输出与输入cat相加时大小不一样了，所以对右边那条道也进行缩放，有两种方法：

- 也使用卷积，使stride=2
- 使用池化进行缩放

我们使用卷积层进行缩放。

再看卷积层，除了第一层将m的输入变成了n的输出之外，其他的卷积层都是输入输出都是n

```python
class Encode_block(nn.Module):
    def __init__(self,in_channel,out_channel):
        super().__init()
        self.con1=conv_block(in_channel,out_channel,stride=2)#第一个卷积层对图像进行缩放
        self.con2=conv_block(out_channel,out_channel)#之后的三层都是out_channel

        self.con3=conv_block(out_channel,out_channel)
        self.con4=conv_block(out_channel,out_channel)

        self.res=conv_block(in_channel,out_channel,stride=2)#残差模块也需要缩放再进相加
    def forward(self,x):
        out1=self.con1(x)
        out1=self.con2(out1)
        short_cut=self.res(x)
        
        out2=self.con3(short_cut+out1)
        out2=self.con4(out2)

        return out1+out2#最后哦返回俩个输出相加
```



##### 构建Decode模块

卷积+反卷积+卷积

先看一下结构特点，发现

- 只有三层而且是顺序的结构
- 输入输出参数的变化：
  - 第一个卷积层卷积核大小为1X1，输入是m,输出为m/4,
  - 第二个反卷积层卷积核大小为3x3,输入输出维数都是m/4，但对图像进行了两倍的放大
  - 最后一层卷积卷积核大小为1x1,输入为m/4输出为n

```python
class Decode_block(nn.Module):
    def __init__(self,in_channel,out_channel):
        super().__init()
        self.con1=conv_block(in_channel,in_channel//4,k_size=1,pad=0)
                            #使卷积核的大小为1x1,此时padding就得去掉了否则就相当于把图像的长宽都扩大了2
        self.decon=deconv_block(in_channel//4,in_channel//4)#反卷积模块默认放大到2倍，所以不用去设置
        self.con2=conv_block(in_channel//4,out_channel,k_size=1,pad=0)#第三个卷积层的卷积核也是1x1的
        
    def forward(self,x):
        x=self.con1(x)
        x=self.decon(x)
        x=self.con2(x)
        return x
```





##### 实现整体网络结构

输入->4个Encode->4个Decode->输出

观察网络结构发现对于输入的3通道RGB图像，先使用一个7x7的卷积将3通道输出到64通道，将图像大小缩小一倍，然后是一个3x3的最大池化再将图片缩小一倍，

然后就输入进编码器模块，每经历一个编码器模块图像小一倍，然后每经过一次解码器模块将图像放大一倍

在输出部分用了两次反卷积也放大了两次，使得输出图片和输入图片大小一致，最后输出的通道数就是要分类的分类数

代码如下：

```pytho
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.input_conv=conv_block(3,64,k_size=7,stride=2,pad=3)#这是输入部分的卷积
        self.input_maxpool=nn.MaxPool2d(kernel_size=(2,2))#对原图进行缩放

        self.encode1=Encode_block(64,64)
        self.encode2=Encode_block(64,128)
        self.encode3=Encode_block(128,256)
        self.encode4=Encode_block(256,512)

        self.decode4=Decode_block(512,256)#这样decode4和encode3的结果是一样的，可以相加
        self.decode3=Decode_block(256,128)
        self.decode2=Decode_block(128,64)
        self.decode1=Decode_block(64,64)

        self.deconv_out1=deconv_block(64,32)#反卷积模块默认放大2倍
        self.conv_out=conv_block(32,32)
        self.deconv_out2=deconv_block(32,2,k_size=2,pad=0,output_padding=0)#因为我们是二分类问题，所以最后输出两个通道
                                                            #因为反卷积核的大小变成了2 所以为了使图像不变，将padding和output_padding都改为0（会变成两倍但像素不进行加减）

    def forward(self,x):
        x=self.input_conv(x)
        x=self.input_maxpool(x)

        e1=self.encode1(x)
        e2=self.encode2(e1)
        e3=self.encode3(e2)
        e4=self.encode4(e3)#使用不同参数的原因是要将他们保留下来以便之后的合并

        d4=self.decode4(e4)+e3
        d3=self.decode3(d4)+e2
        d2=self.decode2(d3)+e1
        d1=self.decode1(d2)

        f1=self.deconv_out1(d1)
        f2=self.conv_out(f1)
        f3=self.deconv_out2(f2,is_act=False)#最后一层就直接输出了，不需要再进行BN以及激活了
        
        return  f3
```

然后就可以进行损失函数的以及优化函数的编写。因为是二分类，所以使用`nn.CrossEntropyLoss()`

在训练过程中增加了一个IOU指标，使用了IOU的一个定义，使用图像面积的交并比，下面是更改后的fit函数的代码

```python
def fit(epoch,model,trainloader,textloader):   #因为这个函数没有输入train_x和train_y所以直接用train_dl和text_dl来算正确率
    correct=0                            #通过记录预测正确的个数和样本总数之间的比值来看正确率
    total=0                              #总样本的个数
    running_loss=0
    epoch_IOU=[]
    model.train()      #使用dropout层和BN层时一定要加这一句表明模型正处于训练状态下  
    for x,y in trainloader:  
        x,y=x.to('cuda'),y.to('cuda') #如果要使用GPU进行加速计算就打开屏蔽 
        y_pred=model(x)
        loss=loss_fn(y_pred,y)
        optim.zero_grad()
        loss.backward()
        optim.step()
        with torch.no_grad():                  #每一个y值进行一次计算，这是对训练数据
            y_pred=torch.argmax(y_pred,dim=1)
            correct+=(y_pred==y).sum().item()  #每一次正确的个数加到correct里
            total+=y.size(0)                   #每一批样本的个数
            running_loss+=loss.item()          #每一批样本的loss

            intersection=torch.logical_and(y,y_pred)#使用torch内置的与运算计算预测值与实际值之间的交集
            union=torch.logical_or(y,y_pred)#使用torch内置的交运算计算预测值与实际值之间的并集
            batch_IOU=torch.true_divide(torch.sum(intersection.cpu()),torch.sum(union.cpu()))
                                    #因为这些都是tensor类型，所以使用torch的求和 和相除方法
                                    #因为之后要求一个epoch中所有IOU的平均值，所以将其放在cpu上这样才能转换成numpy再求平均值
            epoch_IOU.append(batch_IOU)


        
    #exp_lr_scheduler.step() #如果定义了学习速率衰减可以打开这个屏蔽
    
    epoch_acc=correct/(total*256*256)                #正确率为正确个数除以样本个数
    epoch_loss=running_loss/len(trainloader.dataset)  #平均loss为一个批次的总loss除以训练的长度
    
    
    text_correct=0                          #这是对测试数据 ,对于测试数据不用反向传播不用优化
    text_total=0                              
    text_running_loss=0
    test_epoch_IOU=[]
    model.eval()      #告诉模型现在是预测模式，dropout层不需要发挥作用  
    with torch.no_grad():                  
            for x,y in textloader: 
                x,y=x.to('cuda'),y.to('cuda') #如果要使用GPU进行加速计算就打开屏蔽 
                y_pred=model(x)
                loss=loss_fn(y_pred,y)
                y_pred=torch.argmax(y_pred,dim=1)
                text_correct+=(y_pred==y).sum().item() 
                text_total+=y.size(0)                   
                text_running_loss+=loss.item()
                intersection=torch.logical_and(y,y_pred)#使用torch内置的与运算计算预测值与实际值之间的交集
                union=torch.logical_or(y,y_pred)#使用torch内置的交运算计算预测值与实际值之间的并集
                batch_IOU=torch.true_divide(torch.sum(intersection.cpu()),torch.sum(union.cpu()))
                test_epoch_IOU.append(batch_IOU)
    
    epoch_text_acc=text_correct/(text_total*256*256)               
    epoch_text_loss=running_loss/len(textloader.dataset)  
    
    print('epoch: ',epoch,'loss: ',round(epoch_loss,3),
                            'accuracy: ',round(epoch_acc,3),
                            'train_IOU:',round(np.mean(epoch_IOU),3),#在每一个批次结果相同时可以这么写
                            'text_loss: ',round(epoch_text_loss,3),
                            'text_accuracy: ',round(epoch_text_acc,3),
                            'test_IOU:',round(np.mean(test_epoch_IOU),3))
                            
    return epoch_loss,epoch_acc,epoch_IOU,epoch_text_loss,epoch_text_acc,test_epoch_IOU  
                                                #这样就会将每个epoch上的这些平均正确率和平均损失返回
```

然后就是调用fit函数进行训练

```python
train_loss=[] #创建几个空的列表来存放数据,方便绘图，看看模型怎么样
train_acc=[]
test_loss=[]
test_acc=[]
train_iou=[]
test_iou=[]
epochs=50

for epoch in range(epochs):
    epoch_loss,epoch_acc,
    epoch_IOU,epoch_test_loss,epoch_test_acc,test_epoch_IOU=fit(epoch,model,train_dl,test_dl)
    
    train_loss.append(epoch_loss)
    train_acc.append(epoch_acc)
    train_iou.append(epoch_IOU)
    test_loss.append(epoch_test_loss)
    test_acc.append(epoch_test_acc)
    test_iou.append(test_epoch_IOU)
```

以下为训练过程

![image-20211009144713825](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211009144713825.png)

下面是u-net 的训练情况

![image-20211009181246842](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211009181246842.png)

可以看出linknet节约了大量的时间

在参数量方面也是少了一半多

![image-20211009181341767](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211009181341767.png)

![image-20211009181421812](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211009181421812.png)

##  全卷积网络（FCN）

### **CNN 与 FCN**

通常CNN网络在卷积层之后会接上若干个全连接层, 将卷积层产生的特征图(feature map)映射成一个固定长度的特征向量。以AlexNet为代表的经典CNN结构适合于图像级的分类和回归任务，因为它们最后都期望得到整个输入图像的一个数值描述（概率），比如AlexNet的ImageNet模型输出一个1000维的向量表示输入图像属于每一类的概率(softmax归一化)。

栗子：下图中的猫, 输入AlexNet, 得到一个长为1000的输出向量, 表示输入图像属于每一类的概率, 其中在“tabby cat”这一类统计概率最高。

![这里写图片描述](http://img.blog.csdn.net/20161022110633544)

FCN对图像进行像素级的分类，从而解决了语义级别的图像分割（semantic segmentation）问题。与经典的CNN在卷积层之后使用全连接层得到固定长度的特征向量进行分类（全联接层＋softmax输出）不同，**FCN可以接受任意尺寸的输入图像**，采用**反卷积层**对最后一个卷积层的feature map进行上采样, 使它恢复到输入图像相同的尺寸，从而可以对每个像素都产生了一个预测, 同时保留了原始输入图像中的空间信息, 最后在上采样的特征图上进行逐像素分类。

最后逐个像素计算softmax分类的损失, 相当于每一个像素对应一个训练样本。下图是Longjon用于语义分割所采用的全卷积网络(FCN)的结构示意图：

![这里写图片描述](http://img.blog.csdn.net/20161022111939034)

简单的来说，FCN与CNN的区域在把于CNN最后的全连接层换成卷积层，输出的是一张已经Label好的图片。

![这里写图片描述](http://img.blog.csdn.net/20161022115412312)

其实，CNN的强大之处在于它的多层结构能自动学习特征，并且可以学习到多个层次的特征：**较浅的卷积层感知域较小，学习到一些局部区域的特征；较深的卷积层具有较大的感知域，能够学习到更加抽象一些的特征**。这些抽象特征对物体的大小、位置和方向等敏感性更低，从而有助于识别性能的提高。下图CNN分类网络的示意图：

![这里写图片描述](http://img.blog.csdn.net/20161022113201070)

这些抽象的特征对分类很有帮助，可以很好地判断出一幅图像中包含什么类别的物体，但是因为丢失了一些物体的细节，不能很好地给出物体的具体轮廓、指出每个像素具体属于哪个物体，因此做到精确的分割就很有难度。

**传统的基于CNN的分割方法**：为了对一个像素分类，使用该像素周围的一个图像块作为CNN的输入用于训练和预测。这种方法有几个缺点：一是存储开销很大。例如对每个像素使用的图像块的大小为15x15，然后不断滑动窗口，每次滑动的窗口给CNN进行判别分类，因此则所需的存储空间根据滑动窗口的次数和大小急剧上升。二是计算效率低下。相邻的像素块基本上是重复的，针对每个像素块逐个计算卷积，这种计算也有很大程度上的重复。三是像素块大小的限制了感知区域的大小。通常像素块的大小比整幅图像的大小小很多，只能提取一些局部的特征，从而导致分类的性能受到限制。

而全卷积网络(FCN)则是从抽象的特征中恢复出每个像素所属的类别。即从图像级别的分类进一步延伸到像素级别的分类。

### **全连接层 -> 成卷积层**

全连接层和卷积层之间唯一的不同就是卷积层中的神经元只与输入数据中的一个局部区域连接，并且在卷积列中的神经元共享参数。然而在两类层中，神经元都是计算点积，所以它们的函数形式是一样的。因此，将此两者相互转化是可能的：

- 对于任一个卷积层，都存在一个能实现和它一样的前向传播函数的全连接层。权重矩阵是一个巨大的矩阵，除了某些特定块，其余部分都是零。而在其中大部分块中，元素都是相等的。
- 相反，任何全连接层都可以被转化为卷积层。比如，一个 K=4096 的全连接层，输入数据体的尺寸是 7∗7∗512，这个全连接层可以被等效地看做一个 F=7,P=0,S=1,K=4096 的卷积层。换句话说，就是将滤波器的尺寸设置为和输入数据体的尺寸一致了。因为只有一个单独的深度列覆盖并滑过输入数据体，所以输出将变成 1∗1∗4096，这个结果就和使用初始的那个全连接层一样了。

**全连接层转化为卷积层**：在两种变换中，将全连接层转化为卷积层在实际运用中更加有用。假设一个卷积神经网络的输入是 224x224x3 的图像，一系列的卷积层和下采样层将图像数据变为尺寸为 7x7x512 的激活数据体。AlexNet使用了两个尺寸为4096的全连接层，最后一个有1000个神经元的全连接层用于计算分类评分。我们可以将这3个全连接层中的任意一个转化为卷积层：

- 针对第一个连接区域是[7x7x512]的全连接层，令其滤波器尺寸为F=7，这样输出数据体就为[1x1x4096]了。
- 针对第二个全连接层，令其滤波器尺寸为F=1，这样输出数据体为[1x1x4096]。
- 对最后一个全连接层也做类似的，令其F=1，最终输出为[1x1x1000]

实际操作中，每次这样的变换都需要把全连接层的权重W重塑成卷积层的滤波器。那么这样的转化有什么作用呢？它在下面的情况下可以更高效：让卷积网络在一张更大的输入图片上滑动，得到多个输出，这样的转化可以让我们在单个向前传播的过程中完成上述的操作。

举个栗子：如果我们想让224×224尺寸的浮窗，以步长为32在384×384的图片上滑动，把每个经停的位置都带入卷积网络，最后得到6×6个位置的类别得分。上述的把全连接层转换成卷积层的做法会更简便。如果224×224的输入图片经过卷积层和下采样层之后得到了[7x7x512]的数组，那么，384×384的大图片直接经过同样的卷积层和下采样层之后会得到[12x12x512]的数组。然后再经过上面由3个全连接层转化得到的3个卷积层，最终得到[6x6x1000]的输出((12 – 7)/1 + 1 = 6)。这个结果正是浮窗在原图经停的6×6个位置的得分！

> 面对384×384的图像，让（含全连接层）的初始卷积神经网络以32像素的步长独立对图像中的224×224块进行多次评价，其效果和使用把全连接层变换为卷积层后的卷积神经网络进行一次前向传播是一样的。
>
> Evaluating the original ConvNet (with FC layers) independently across 224x224 crops of the 384x384 image in strides of 32 pixels gives an identical result to forwarding the converted ConvNet one time.

如下图所示，FCN将传统CNN中的全连接层转化成卷积层，对应CNN网络FCN把最后三层全连接层转换成为三层卷积层。在传统的CNN结构中，前5层是卷积层，第6层和第7层分别是一个长度为4096的一维向量，第8层是长度为1000的一维向量，分别对应1000个不同类别的概率。FCN将这3层表示为卷积层，卷积核的大小 (通道数，宽，高) 分别为 (4096,1,1)、(4096,1,1)、(1000,1,1)。看上去数字上并没有什么差别，但是卷积跟全连接是不一样的概念和计算过程，使用的是之前CNN已经训练好的权值和偏置，但是不一样的在于权值和偏置是有自己的范围，属于自己的一个卷积核。因此FCN网络中所有的层都是卷积层，故称为全卷积网络。

![这里写图片描述](http://img.blog.csdn.net/20161022113129275)

下图是一个全卷积层，与上图不一样的是图像对应的大小下标，CNN中输入的图像大小是同意固定resize成 227x227 大小的图像，第一层pooling后为55x55，第二层pooling后图像大小为27x27，第五层pooling后的图像大小为13*13。而FCN输入的图像是H*W大小，第一层pooling后变为原图大小的1/4，第二层变为原图大小的1/8，第五层变为原图大小的1/16，第八层变为原图大小的1/32（勘误：其实真正代码当中第一层是1/2，以此类推）。

![这里写图片描述](http://img.blog.csdn.net/20161022113148603)

经过多次卷积和pooling以后，得到的图像越来越小，分辨率越来越低。其中图像到 H/32∗W/32 的时候图片是最小的一层时，所产生图叫做heatmap热图，热图就是我们最重要的高维特诊图，得到高维特征的heatmap之后就是最重要的一步也是最后的一步对原图像进行upsampling，把图像进行放大、放大、放大，到原图像的大小。

![这里写图片描述](http://img.blog.csdn.net/20161022113100758)

最后的输出是1000张heatmap经过upsampling变为原图大小的图片，为了对每个像素进行分类预测label成最后已经进行语义分割的图像，这里有一个小trick，就是最后通过逐个像素地求其在1000张图像该像素位置的最大数值描述（概率）作为该像素的分类。因此产生了一张已经分类好的图片，如下图右侧有狗狗和猫猫的图。

![这里写图片描述](http://img.blog.csdn.net/20161022113114585)

### **upsampling**

相较于使用被转化前的原始卷积神经网络对所有36个位置进行迭代计算，使用转化后的卷积神经网络进行一次前向传播计算要高效得多，因为36次计算都在共享计算资源。这一技巧在实践中经常使用，一次来获得更好的结果。比如，通常将一张图像尺寸变得更大，然后使用变换后的卷积神经网络来对空间上很多不同位置进行评价得到分类评分，然后在求这些分值的平均值。

最后，如果我们想用步长小于32的浮窗怎么办？用多次的向前传播就可以解决。比如我们想用步长为16的浮窗。那么先使用原图在转化后的卷积网络执行向前传播，然后分别沿宽度，沿高度，最后同时沿宽度和高度，把原始图片分别平移16个像素，然后把这些平移之后的图分别带入卷积网络。

![这里写图片描述](http://img.blog.csdn.net/20161024115403020)

如下图所示，当图片在网络中经过处理后变成越小的图片，其特征也越明显，就像图像中颜色所示，当然啦，最后一层的图片不再是一个1个像素的图片，而是原图像 H/32xW/32 大小的图，这里为了简化而画成一个像素而已。

![这里写图片描述](http://img.blog.csdn.net/20161022113244839)

如下图所示，对原图像进行卷积conv1、pool1后原图像缩小为1/2；之后对图像进行第二次conv2、pool2后图像缩小为1/4；接着继续对图像进行第三次卷积操作conv3、pool3缩小为原图像的1/8，此时保留pool3的featureMap；接着继续对图像进行第四次卷积操作conv4、pool4，缩小为原图像的1/16，保留pool4的featureMap；最后对图像进行第五次卷积操作conv5、pool5，缩小为原图像的1/32，然后把原来CNN操作中的全连接变成卷积操作conv6、conv7，图像的featureMap数量改变但是图像大小依然为原图的1/32，此时图像不再叫featureMap而是叫heatMap。

现在我们有1/32尺寸的heatMap，1/16尺寸的featureMap和1/8尺寸的featureMap，1/32尺寸的heatMap进行upsampling操作之后，因为这样的操作还原的图片仅仅是conv5中的卷积核中的特征，限于精度问题不能够很好地还原图像当中的特征，因此在这里向前迭代。把conv4中的卷积核对上一次upsampling之后的图进行反卷积补充细节（相当于一个差值过程），最后把conv3中的卷积核对刚才upsampling之后的图像进行再次反卷积补充细节，最后就完成了整个图像的还原。

![这里写图片描述](http://img.blog.csdn.net/20161022113219788)

### 缺点

在这里我们要注意的是FCN的缺点：

1. 是得到的结果还是不够精细。进行8倍上采样虽然比32倍的效果好了很多，但是上采样的结果还是比较模糊和平滑，对图像中的细节不敏感。
2. 是对各个像素进行分类，没有充分考虑像素与像素之间的关系。忽略了在通常的基于像素分类的分割方法中使用的空间规整（spatial regularization）步骤，缺乏空间一致性。1
# 常用术语

### 上采样，下采样：

​	缩小图像（或称为下采样（subsampled）或降采样（downsampled））的主要目的有两个：1、使得图像符合显示区域的大小；2、生成对应图像的缩略图。
​    放大图像（或称为上采样（upsampling）或图像插值（interpolating））的主要目的是放大原图像,从而可以显示在更高分辨率的显示设备上。对图像的缩放操作并不能带来更多关于该图像的信息, 因此图像的质量将不可避免地受到影响。然而，确实有一些缩放方法能够增加图像的信息，从而使得缩放后的图像质量超过原图质量的。

- 下采样原理：对于一幅图像I尺寸为M*N，对其进行s倍下采样，即得到(M/s)*(N/s)尺寸的得分辨率图像，当然s应该是M和N的公约数才行，如果考虑的是矩阵形式的图像，就是把原始图像s*s窗口内的图像变成一个像素，这个像素点的值就是窗口内所有像素的均值：
- 上采样原理：图像放大几乎都是采用内插值方法，即在原有图像像素的基础上在像素点之间采用合适的插值算法插入新的元素。

> 在应用在计算机视觉的深度学习领域，由于输入图像通过卷积神经网络(CNN)提取特征后，输出的尺寸往往会变小，而有时我们需要将图像恢复到原来的尺寸以便进行进一步的计算(e.g.:图像的语义分割)，这个采用扩大图像尺寸，实现图像由小分辨率到大分辨率的映射的操作，叫做上采样
>
> 上采样有3种常见的方法：双线性插值(bilinear)，反卷积(Transposed Convolution)，反池化(Unpooling)

### 反卷积层：

反卷积，也叫转置卷积，它并不是正向卷积的完全逆过程，用一句话来解释：
反卷积是一种特殊的正向卷积，先按照一定的比例通过补 ![[公式]](https://www.zhihu.com/equation?tex=0) 来扩大输入图像的尺寸，接着旋转卷积核，再进行正向卷积。

在卷积网络里面，我们是通过卷积层操作，获得feature map，然而对于反卷积层，则是通过feature map的卷积操作，输出value，这输出值是我们期待的图像的值。有点像通过反卷积还原feature map到原始图像。可以进行可视化操作，将feature map可视化，看到底学习到了什么。

反卷积的操作只是恢复了输入矩阵 ![[公式]](https://www.zhihu.com/equation?tex=X) 的尺寸大小，并不能恢复 ![[公式]](https://www.zhihu.com/equation?tex=X) 的每个元素值

简单来说就是先扩张填充0再进行一次卷积

- **反卷积的具体计算步骤**

下面我们用一组实验更直观的解释一下在 tensorflow 中反卷积的过程：
我们令输入图像为：
![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+input+%3D+++%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D+++++1+%26+2+%26+3+%5C%5C+++++4+%26+5+%26+6+%5C%5C+++++7+%26+8+%26+9+%5C%5C+++%5Cend%7Barray%7D%5Cright%5D+%5Cend%7Balign%2A%7D)

卷积核为：
![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+kernel+%3D+++%5Cleft%5B%5Cbegin%7Barray%7D%7Bccc%7D+++++1+%26+0+%26+0+%5C%5C+++++0+%26+1+%26+0+%5C%5C+++++0+%26+0+%26+1+%5C%5C+++%5Cend%7Barray%7D%5Cright%5D+%5Cend%7Balign%2A%7D)

**case 1**
如果要使输出的尺寸是 ![[公式]](https://www.zhihu.com/equation?tex=5%5Ctimes+5) ，步数 ![[公式]](https://www.zhihu.com/equation?tex=stride%3D2) 

1. 现根据步数 ![[公式]](https://www.zhihu.com/equation?tex=strides) 对输入的内部进行填充，这里 ![[公式]](https://www.zhihu.com/equation?tex=strides) 可以理解成输入放大的倍数，即在 ![[公式]](https://www.zhihu.com/equation?tex=input) 的每个元素之间填充 ![[公式]](https://www.zhihu.com/equation?tex=0) ，![[公式]](https://www.zhihu.com/equation?tex=0) 的个数 ![[公式]](https://www.zhihu.com/equation?tex=n) 与 ![[公式]](https://www.zhihu.com/equation?tex=strides) 的关系为：
   ![[公式]](https://www.zhihu.com/equation?tex=n%3Dstrides-1)

例如这里举例的 ![[公式]](https://www.zhihu.com/equation?tex=strides%3D2)，即在 ![[公式]](https://www.zhihu.com/equation?tex=input) 的每个元素之间填 ![[公式]](https://www.zhihu.com/equation?tex=1) 个 ![[公式]](https://www.zhihu.com/equation?tex=0) ：
![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+input_%7Bpad%7D+%3D+++%5Cleft%5B%5Cbegin%7Barray%7D%7Bccccc%7D+++++1+%26+0+%26+2+%26+0+%26+3+%5C%5C+++++0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++++4+%26+0+%26+5+%26+0+%26+6+%5C%5C+++++0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++++7+%26+0+%26+8+%26+0+%26+9+%5C%5C+++%5Cend%7Barray%7D%5Cright%5D+%5Cend%7Balign%2A%7D)

因为卷积类型为 same，所以此时， ![[公式]](https://www.zhihu.com/equation?tex=i%3D5%2Ck%3D3%2Cs%3D1%2Cp%3D1) 。

2. 接下来，用卷积核 ![[公式]](https://www.zhihu.com/equation?tex=kernel) 对填充后的输入 ![[公式]](https://www.zhihu.com/equation?tex=input_%7Bpad%7D) 进行步长 ![[公式]](https://www.zhihu.com/equation?tex=stride%3D1) 的正向卷积，根据正向卷积输出尺寸公式： ![[公式]](https://www.zhihu.com/equation?tex=o%3D%5Cfrac%7Bi-k%2B2p%7D%7Bs%7D%2B1%3D%5Cfrac%7B5-3%2B2%5Ctimes1%7D%7B1%7D%2B1%3D5) 得到输出尺寸是 ![[公式]](https://www.zhihu.com/equation?tex=5%5Ctimes+5) ，反卷积公式中我们给出的输出尺寸参数 ![[公式]](https://www.zhihu.com/equation?tex=output%5C_shape) 也是为 ![[公式]](https://www.zhihu.com/equation?tex=5)，两者相同，所以可以进行计算，结果为：
   ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+output+%3D+++%5Cleft%5B%5Cbegin%7Barray%7D%7Bccccc%7D+++++1+%26+0+%26+2+%26+0+%26+3+%5C%5C+++++0+%26+6+%26+0+%26+8+%26+0+%5C%5C+++++4+%26+0+%26+5+%26+0+%26+6+%5C%5C+++++0+%26+12+%26+0+%26+14+%26+0+%5C%5C+++++7+%26+0+%26+8+%26+0+%26+9+%5C%5C+++%5Cend%7Barray%7D%5Cright%5D+%5Cend%7Balign%2A%7D)



如果最后的输出尺寸不是需要的尺寸，还需要继续填充0继续卷积：

卷积类型是 same，我们首先在外围填充一圈 ![[公式]](https://www.zhihu.com/equation?tex=0) ：
![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+input+%3D+++%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccccccc%7D++++++++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C++++++0+%26+1+%26+0+%26+2+%26+0+%26+3+%26+0+%5C%5C++++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C++++++0+%26+4+%26+0+%26+5+%26+0+%26+6+%26+0+%5C%5C++++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C++++++0+%26+7+%26+0+%26+8+%26+0+%26+9+%26+0+%5C%5C++++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++%5Cend%7Barray%7D%5Cright%5D+%5Cend%7Balign%2A%7D)

这时发现，填充后的输入尺寸与 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) 的卷积核卷积后的输出尺寸是 ![[公式]](https://www.zhihu.com/equation?tex=5%5Ctimes+5) ，没有达到 ![[公式]](https://www.zhihu.com/equation?tex=output%5C_shape) 的 ![[公式]](https://www.zhihu.com/equation?tex=6%5Ctimes+6) ，这就需要继续填充 ![[公式]](https://www.zhihu.com/equation?tex=0)，tensorflow 的计算规则是优先在左侧和上侧填充一排 ![[公式]](https://www.zhihu.com/equation?tex=0) ，填充后的输入变为：
![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+input+%3D+++%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccccccc%7D+++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++++0+%26+0+%26+1+%26+0+%26+2+%26+0+%26+3+%26+0+%5C%5C+++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++++0+%26+0+%26+4+%26+0+%26+5+%26+0+%26+6+%26+0+%5C%5C+++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++++0+%26+0+%26+7+%26+0+%26+8+%26+0+%26+9+%26+0+%5C%5C+++++0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%26+0+%5C%5C+++%5Cend%7Barray%7D%5Cright%5D+%5Cend%7Balign%2A%7D)

接下来，再对这个填充后的输入与 ![[公式]](https://www.zhihu.com/equation?tex=3%5Ctimes3) 的卷积核卷积，结果为：
![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+input+%3D+++%5Cleft%5B%5Cbegin%7Barray%7D%7Bcccccc%7D+++++1+%26+0+%26+2+%26+0+%26+3+%26+0+%5C%5C+++++0+%26+1+%26+0+%26+2+%26+0+%26+3+%5C%5C+++++4+%26+0+%26+6+%26+0+%26+8+%26+0+%5C%5C+++++0+%26+4+%26+0+%26+5+%26+0+%26+6+%5C%5C+++++7+%26+0+%26+12+%26+0+%26+14+%26+0+%5C%5C+++++0+%26+7+%26+0+%26+8+%26+0+%26+9+%5C%5C+++%5Cend%7Barray%7D%5Cright%5D+%5Cend%7Balign%2A%7D)

> 反卷积不能恢复数值，而且，在当 ![[公式]](https://www.zhihu.com/equation?tex=stride%5Cgeq2) 时，即便使用完全相同的参数进行转置卷积，输入的尺寸也不能恢复。

###  超参数

​	在机器学习的上下文中，超参数是在开始学习过程之前设置值的参数，而不是通过训练得到的参数数据。通常情况下，需要对超参数进行优化，给学习机选择一组最优超参数，以提高学习的性能和效果。

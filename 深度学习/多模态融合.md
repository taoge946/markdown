# 多模态融合

## 多模态学习

为了使人工智能进一步加强对我们周边事物的理解，它需要具备解释多模态信号的能力。一般多模态需要处理的任务主要如上图有：

- 表征（Representation）。找到某种对多模态信息的统一表示，分Coordinated representations（每个模态各自映射然后用用相关度距离来约束表示），Joint representations（多个模态一起映射）。
- 翻译（Translation）。一个模态映射到另一个模态，分example-based（有候选集，如检索任务），generative（Decoder-Encoder）。
- 对齐（Alignment）。找模态子成份之间的关系，如某词对应某区域。分显式对齐和隐式对齐，Attention首当其冲。
- **融合（Fusion）**。整合信息。分model-agnostic（早晚融合），model-based（融合更深入），也是本篇要整理的内容。
- 联合学习（Co-learning）。通过利用丰富的模态的知识来辅助稀缺的模态，分parallel（如迁移学习），non-parallel（迁移学习，zero shot），hybrid。

接下来重点介绍一些多模态融合方法。

## **多模态融合 （Multimodal Fusion）**

一般来说，模态是指事物发生或存在的方式，多模态是指两个或者两个以上的模态的各种形式的组合。对每一种信息的来源或者形式，都可以称为一种模态（Modality），目前研究领域中主要是对图像，文本，语音三种模态的处理。之所以要对模态进行融合，是因为不同模态的表现方式不一样，看待事物的角度也会不一样，所以存在一些交叉（所以存在信息冗余），互补（所以比单特征更优秀）的现象，甚至模态间可能还存在多种不同的信息交互，如果能合理的处理多模态信息，就能得到丰富特征信息。即概括来说多模态的显著特点是： 冗余性 和 互补性 。
![img](https://img-blog.csdnimg.cn/20200327211919587.png#pic_center)

传统特征融合算法主要可以分为三类：1.基于贝叶斯决策理论的算法 2.基于稀疏表示理论的算法 3.基于深度学习理论算法。传统方法不做整理，其中的深度学习方法按照融合的层次从下到上每一层都可以fusion（融合）：

- pixel level。对原始数据最小粒度进行融合。
- feature level 。对抽象的特征进行融合，这也是用的最多的。包括early 和 late fusion，代表融合发生在特征抽取的早期和晚期，如上图。early是指先将特征融合后（concat、add）再输出模型，缺点是无法充分利用多个模态数据间的互补性，且存在信息冗余问题（可由PCA，AE等方法缓解）。late分融合和不融合两种形式，不融合有点像集成学习，不同模态各自得到的结果了之后再统一打分进行融合，好处是模型独立鲁棒性强。融合的方式即在特征生成过程中（如多层神经网络的中间）进行自由的融合，灵活性比较高，如金字塔融合。
- decision level 对决策结果进行融合，这就和集成学习很像了
- hybrid。混合融合多种融合方法。

![img](https://img-blog.csdnimg.cn/20200327204248541.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70#pic_center)

就融合的详细方法上，可以基于

- 基于矩阵
- 基于普通神经网络
- 基于生成模型
- 基于注意力
- 其他网络模型如NAS，GAN，Graph等
- 融合矩阵和特征
- shuffle和shift等不需要额外参数的方法。

### **TFN(Multimodal Tensor Fusion Network)**多模态张量融合网络

首先是基于矩阵的TFN，TFN属于early fusion，是一个典型通过矩阵运算进行融合特征融合的多模态网络，即直接对三种模态的数据（如Text，Image，Audio）的三个特征向量X，Y，Z，进行：

![image-20210915143609092](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210915143609092.png)

便得到了融合后的结果m，如下图：

![img](https://img-blog.csdnimg.cn/20200327213457120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70#pic_center)

缺点：TFN通过模态之间的张量外积（Outer product）计算不同模态的元素之间的相关性，但会极大的增加特征向量的维度，造成模型过大，难以训练。

### **Low-rank Multimodal Fusion** (低秩多模融合)

出自论文 Efficient Low-rank Multimodal Fusion with Modality-Specific Factors，ACL2018。是TFN的等价升级版，就具体模型如图。LMF利用对权重进行低秩矩阵分解，将TFN先张量外积再FC的过程变为每个模态先单独线性变换之后再多维度点积，可以看作是多个低秩向量的结果的和，从而减少了模型中的参数数量。
![img](https://img-blog.csdnimg.cn/20200328190202905.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70#pic_center)

缺点：虽然是TFN的升级，但一旦特征过长，仍然容易参数爆炸。

### **PTP (polynomialtensor pooling)**（多项式传感器池）

出自论文，Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling，NIPS 2019.
以往的双线性或三线性池融合的能力有限，不能释放多线性融合的完全表现力和受限的交互顺序。 更重要的是，简单地同时融合特征忽略了复杂的局部相互关系。所以升级为一个多项式张量池(PTP)块，通过考虑高阶矩来集成多模态特征。即将concat的模型x N之后再做一个低秩分解。
![img](https://img-blog.csdnimg.cn/20200328190941751.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70#pic_center)

### **DSSM（Deep Structured Semantic Models）**

DSSM是搜索领域的模型，属于late fusion。它通过用 DNN 把 Query 和 Title（换成不同的模态数据就行） 表达为低维语义向量，并通过 cosine 距离来计算两个语义向量的距离，最终训练出语义相似度模型。该模型既可以用来预测语义相似度，又可以获得某模态的低维语义向量表达。（可以将两个模态约束至统一表示空间，多模态协同表示。与之对应的多模态联合表示是先concat再FC）
![img](https://img-blog.csdnimg.cn/20200327213236933.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70#pic_center)

其他玩法：可以在DNN的过程中，用recurrent residual fusion (RRF) ，多个残差，然后将3次recurrent的结果与最开始的输入concat起来，最后将concat得到的结果做融合。

### **Dynamic Fusion for Multimodal Data**

以上的融合方法都太过“生硬”，能否有更好更自然的融合方法呢？AE（autoencoder）首当其冲，如图左图，先把所有模态fc即encoder，再用decode还原特征，最后计算特征之间的损失。

有了AE，GAN的出现不会太迟。如图右图，将不采用固定的模态融合方法，而是自动学习“how”融合。先对video和speech转换完成后，和text进行对抗（video往往和speech是搭配的，所以先融合）。
![img](https://img-blog.csdnimg.cn/20200328191409565.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70#pic_center)

另外在CVPR2019，有一篇–MFAS：Multimodal Fusion Architecture Search，首次用[神经架构搜索](https://blog.csdn.net/qq_39388410/article/details/96113698)做how融合。

### **MFN(Memory Fusion Network)**

出自Memory Fusion Network for Multi-View Sequential Learning，AAAI 2018。
17,18年是注意力机制开始统治学术界的一年，很多工作都做了这方面的工作。MFN就是一种使用“Delta-memory attention”和“Multi-View Gated Memory”来同时捕捉时序上和模态间的交互，以得到更好的多视图融合。模型图如下，用memory的目的是能保存上一时刻的多模态交互信息，gated过滤，Attention分配权重。
![img](https://img-blog.csdnimg.cn/20200328191557282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70#pic_center)

### **淘宝视频多模态应用**

淘宝视频的多模态信息也是十分丰富，而用LMF和TFN参数量往往会爆炸，就不得已要先将每个模态特征降维，然而降维本身是有损的，导致降维后的模态特征再外积不如直接利用不同模态间特征拼接。

不过不要紧，淘宝也就提出了基于Modal Attention的多模态特征融合方法。Modal Attention是用法是，预测基于concat后的多模态联合特征对不同模态的重要性分布概率，再将分布概率与多模态融合特征做点积，得到对于不同模态特征重要性重新加权过后的新的多模态融合特征。
![img](https://img-blog.csdnimg.cn/20200327195933425.jpg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM5Mzg4NDEw,size_16,color_FFFFFF,t_70)
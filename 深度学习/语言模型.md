# 语言模型

本文总结自近来对语言模型的调研，有空会不断参考完善。大家有什么建议和意见都可以在评论区指出，一起讨论交流，谢谢大家。

## 1. 定义

- 标准定义：对于语言序列 ![[公式]](https://www.zhihu.com/equation?tex=w_1%2Cw_2%2C...%2Cw_n)，语言模型就是计算该序列的概率，即 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%2C+w_2%2C+...%2Cw_n%29) 。
- 从机器学习的角度来看：语言模型是对语句的**概率分布**的建模。
- 通俗解释：判断一个语言序列是否是正常语句，即是否是**人话，**例如 ![[公式]](https://www.zhihu.com/equation?tex=P%28I+%5C+am+%5C+Light%29%3EP%28Light+%5C+I+%5C+am%29) 。

## 2. 统计语言模型

## 2.1 n-gram 语言模型的基本知识

首先，由链式法则(chain rule)可以得到

![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%2Cw_2%2C...%2Cw_n%29%3DP%28w_1%29P%28w_2%7Cw_1%29%5Ccdot%5Ccdot%5Ccdot+P%28w_n%7Cw_1%2C...%2Cw_%7Bn-1%7D%29)

在统计语言模型中，采用极大似然估计来计算每个词出现的条件概率，即

![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+P%28w_i%7Cw_1%2C...%2Cw_%7Bi-1%7D%29%26%3D%5Cfrac%7BC%28w_1%2Cw_2%2C...%2Cw_i%29%7D%7B%5Csum_%7Bw%7DC%28w_1%2Cw_2%2C..w_%7Bi-1%7D%2Cw%29%7D%5C%5C+%26%5Coverset%7B%5Ctext%7B%3F%7D%7D%7B%3D%7D%5Cfrac%7BC%28w_1%2Cw_2%2C...%2Cw_i%29%7D%7BC%28w_1%2Cw_2%2C..w_%7Bi-1%7D%29%7D+%5Cend%7Balign%7D+)

其中，![[公式]](https://www.zhihu.com/equation?tex=C%28%5Ccdot%29) 表示子序列在训练集中出现的次数。注意，上式存在一个**细节**，很多讲义上都是默认第二步直接等式成立。是否是这样呢，先埋一个坑，看官请仔细往下看。

对于任意长的自然语言语句，根据极大似然估计直接计算 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_1%2C...%2Cw_%7Bi-1%7D%29) 显然不现实。

为了解决这个问题，我们引入**马尔可夫假设**(Markov assumption)，即假设当前词出现的概率只依赖于前 ![[公式]](https://www.zhihu.com/equation?tex=n-1) 个词，可以得到

![[公式]](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_1%2Cw_2%2C...%2Cw_%7Bi-1%7D%29%3DP%28w_i%7Cw_%7Bi-n%2B1%7D%2C...%2Cw_%7Bi-1%7D%29)

基于上式，定义 **n-gram** 语言模型如下：

n=1 unigram：![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%2Cw_2%2C...%2Cw_n%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28w_i%29)

n=2 bigram： ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%2Cw_2%2C...%2Cw_n%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28w_i%7Cw_%7Bi-1%7D%29)

n=3 trigram： ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%2Cw_2%2C...%2Cw_n%29%3D%5Cprod_%7Bi%3D1%7D%5E%7Bn%7DP%28w_i%7Cw_%7Bi-2%7D%2Cw_%7Bi-1%7D%29)

...

其中， 当 ![[公式]](https://www.zhihu.com/equation?tex=n%3E1) 时，为了使句首词的条件概率有意义，需要给原序列加上一个或多个**起始符** ![[公式]](https://www.zhihu.com/equation?tex=%3Cs%3E) 。可以说起始符的作用就是为了**表征句首词出现的条件概率**。

> 我不清楚大家第一次见到 bigram 概率连乘的时候是怎么考虑的。因为如果只是对chain rule后面的条件概率采用Markov assumption，那应该没有 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%29) 什么事啊！
> 我个人开始理解的角度以为单纯的是为了把 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%29) 纳入连乘的形式，所以给全部句子都加入一个起始符。但是如果细想不难发现 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=P%28w_1%7C%3Cs%3E%29) 背后的意义完全不同。

好了，以 bigram 为例，现在的序列变成了![[公式]](https://www.zhihu.com/equation?tex=%3Cs%3E%2Cw_1%2Cw_2%2C...%2Cw_n)。看到这里，肯定有人会问，n-gram 中常见的序列形式不应该还有个**结束符** ![[公式]](https://www.zhihu.com/equation?tex=%3C%2Fs%3E) 嘛，那结束符又是用来干嘛的呢？

> 很多书上对此的解释都是一笔带过，说加上结束符</s>是为了使所有句子的概率和为1。
> 我们不禁要思考的是这句话正确吗，难道不加结束符概率就不是1了吗？
> 带着这个问题以及前面埋的坑，继续往下。

考虑一个简单却直观的情形，假设词表 ![[公式]](https://www.zhihu.com/equation?tex=V%3D%5C%7Ba%2C%5C+b%5C%7D) ，训练集给定如下

> 共四个序列，每个序列的长度为2(不包括起始符)
> ![[公式]](https://www.zhihu.com/equation?tex=W_1%3A~~%3Cs%3E~a~~b)
> ![[公式]](https://www.zhihu.com/equation?tex=W_2%3A~~%3Cs%3E~b~~b)
> ![[公式]](https://www.zhihu.com/equation?tex=W_3%3A~~%3Cs%3E~b~~a)
> ![[公式]](https://www.zhihu.com/equation?tex=W_4%3A~~%3Cs%3E~a~~a)
> 注：该例取自[Speech and Language Processing 3rd ed](https://link.zhihu.com/?target=https%3A//web.stanford.edu/~jurafsky/slp3/)

不难发现，上述四个序列已经囊括了词表 ![[公式]](https://www.zhihu.com/equation?tex=V) 下长度为2的所有可能序列。

**不考虑结束符**，先利用上述极大似然估计的第一个等式可直接计算出

![[公式]](https://www.zhihu.com/equation?tex=P%28a%7C%3Cs%3E%29%3D1%2F2%2C~P%28b%7C%3Cs%3E%29%3D1%2F2+)

![[公式]](https://www.zhihu.com/equation?tex=P%28a%7Ca%29%3D1%2F2%2C~P%28a%7Cb%29%3D1%2F2%2C~P%28b%7Ca%29%3D1%2F2%2C~P%28b%7Cb%29%3D1%2F2)

然后，基于 bigram 语言模型，惊奇的发现

![[公式]](https://www.zhihu.com/equation?tex=P%28W_1%29%2BP%28W_2%29%2BP%28W_3%29%2BP%28W_4%29%3D1)

**What?** 说好的语言模型是建模**所有句子**的概率分布呢，怎么对于长度为2的序列，所有的概率和就满足1了，这还给不给其他长度的序列活路了？

> 感兴趣的同学可以尝试一下，同样词表 ![[公式]](https://www.zhihu.com/equation?tex=V) 下，训练集包含所有序列长度为3的情形。

这里直接给出**结论**：

- 当不加结束符时，n-gram 语言模型只能分别对所有**固定长度**的序列进行概率分布建模，而不是任意长度的序列。
- 出现这个现象的原因就是，上述极大似然估计的第二个等式只有在序列包含结束符的时候才成立。(填坑了)
- 综合上述两点，解释和印证了添加结束符的作用。

## 2.2 n-gram 语言模型中的平滑技术

由于平滑(Smoothing)这块内容网上介绍的都还挺详细的，并且我个人对平滑技术并无新的理解，故不打算做详细展开，忘谅解。

首先，来看一下，平滑技术是干嘛的？

一方面，我们知道自然语言处理中的一大痛点就是出现**未登录词**(OOV)的问题，即测试集中出现了训练集中未出现过的词，导致语言模型计算出的概率为零。另一方面，可能某个子序列未在训练集中出现，也会导致概率为零。而这显然不是我们乐意看到的，平滑的出现就是为了缓解这类问题。

其次，常见的平滑技术有：

1. Laplace Smoothing
2. Interpolation
3. Kneser-Ney
4. ...

## 2.3 n-gram 语言模型小结

总结下基于统计的 n-gram 语言模型的优缺点：

优点：(1) 采用极大似然估计，参数易训练；(2) 完全包含了前 n-1 个词的全部信息；(3) 可解释性强，直观易理解。

缺点：(1) 缺乏长期依赖，只能建模到前 n-1 个词；(2) 随着 n 的增大，参数空间呈指数增长；(3) 数据稀疏，难免会出现OOV的问题；(4) 单纯的基于统计频次，泛化能力差。

## 3. 神经网络语言模型

有了上面的基础，我们可以稍微总结下语言模型到底在建模什么，私以为可以看作是在给定一个序列的前提下，预测下一个词出现的概率，即

![[公式]](https://www.zhihu.com/equation?tex=P%28w_i%7Cw_1%2C...%2Cw_%7Bi-1%7D%29)

不论 n-gram 中的 n 怎么选取，实际上都是对上式的近似。理解了这点，就不难理解神经网络语言模型的本质。

## 3.1 基于前馈神经网络的模型

Bengio 在03年的这篇经典 [paper](https://link.zhihu.com/?target=http%3A//www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) 中，提出了如下图所示的前馈神经网络结构

![img](https://pic1.zhimg.com/80/v2-16f0cd6ee4356dae5218c4739faf1e00_720w.jpg)

先给每个词在连续空间中赋予一个向量(词向量)，再通过神经网络去学习这种分布式表征。利用神经网络去建模当前词出现的概率与其前 n-1 个词之间的约束关系。很显然这种方式相比 n-gram 具有更好的**泛化能力**，**只要词表征足够好。**从而很大程度地降低了数据稀疏带来的问题。但是这个结构的明显缺点是仅包含了**有限的前文信息**。

## 3.2 基于循环神经网络的模型

为了解决定长信息的问题，Mikolov 于2010年发表的论文 [Recurrent neural network based language model](https://link.zhihu.com/?target=https%3A//www.isca-speech.org/archive/archive_papers/interspeech_2010/i10_1045.pdf) 正式揭开了循环神经网络(RNN)在语言模型中的强大历程。

> 插一句，注意力机制(attention mechanism)应用在 seq2seq 中也是为了克服 encoder 对任意句子只能给出一个固定size的表征，而这个表征在遇到长句时则显得包含信息量不够。

以序列**“我想你”**为例介绍RNN语言模型的建模过程：

![img](https://pic1.zhimg.com/80/v2-4fb52f5a6c8dc4cd97318ee2de187fc8_720w.jpg)

在统计语言模型中，介绍过需要给序列添加起始符和结束符，神经网络语言模型也不例外。网络的输入层是"<s>我想你"，输出层可以看作是分别计算条件概率 P(w|<s>)、P(w|<s>我)、P(w|<s>我想)、P(w|<s>我想你) 在整个词表![[公式]](https://www.zhihu.com/equation?tex=%7CV%7C)中值。而我们的目标就是使期望词对应的条件概率尽可能大。

相比单纯的前馈神经网络，隐状态的传递性使得RNN语言模型原则上可以捕捉前向序列的所有信息(虽然可能比较弱)。通过在整个训练集上优化交叉熵来训练模型，使得网络能够尽可能建模出自然语言序列与后续词之间的内在联系。

## 3.3 神经网络语言模型小结

神经网络语言模型(NNLM)通过构建神经网络的方式来探索和建模自然语言内在的依赖关系。尽管与统计语言模型的直观性相比，神经网络的黑盒子特性决定了NNLM的可解释性较差，但这并不妨碍其成为一种非常好的概率分布建模方式。

优点：(1) 长距离依赖，具有更强的约束性；(2) 避免了数据稀疏所带来的OOV问题；(3) 好的词表征能够提高模型泛化能力。

缺点：(1) 模型训练时间长；(2) 神经网络黑盒子，可解释性较差。

## 4. 语言模型的评价指标

上面介绍了两大类对语言模型建模的方式，细心的同学已经可能已经考虑到了：既然大家都是对自然语言的概率建模，那么怎么知道谁效果更好呢，谁的模型更加接近真实分布呢？信息论中常采用**相对熵**(relative entropy)来衡量两个分布之间的相近程度。

> 对于离散随机变量X，熵、交叉熵以及相对熵的定义分别如下
> ![[公式]](https://www.zhihu.com/equation?tex=H%28p%29+%3D-%5Csum_i+p%28x_i%29%5Ctext%7Blog%7Dp%28x_i%29)
> ![[公式]](https://www.zhihu.com/equation?tex=H%28p%2Cq%29%3D-%5Csum_i+p%28x_i%29%5Ctext%7Blog%7Dq%28x_i%29)
> ![[公式]](https://www.zhihu.com/equation?tex=D%28p%7C%7Cq%29%3DH%28p%2Cq%29-H%28p%29%3D%5Csum_i+p%28x_i%29%5Ctext%7Blog%7Dp%28x_i%29%2Fq%28x_i%29)
> 其中， ![[公式]](https://www.zhihu.com/equation?tex=p%28x%29) 和 ![[公式]](https://www.zhihu.com/equation?tex=q%28x%29) 都是对随机变量概率分布的建模。

假定 ![[公式]](https://www.zhihu.com/equation?tex=p) 是样本的真实分布，![[公式]](https://www.zhihu.com/equation?tex=q) 是对其的建模。因为真实分布的熵 ![[公式]](https://www.zhihu.com/equation?tex=H%28p%29) 值是确定的，因此优化相对熵 ![[公式]](https://www.zhihu.com/equation?tex=D%28p%7C%7Cq%29) 等价于优化交叉熵 ![[公式]](https://www.zhihu.com/equation?tex=H%28p%2Cq%29+)。这里有个小细节是 ![[公式]](https://www.zhihu.com/equation?tex=H%28p%2Cq%29%5Cgeq+H%28p%29) 恒成立，感兴趣的同学可在[这里](https://link.zhihu.com/?target=https%3A//en.wikipedia.org/wiki/Gibbs%27_inequality)找到具体证明过程。

对于自然语言序列 ![[公式]](https://www.zhihu.com/equation?tex=W%3Dw_1%2Cw_2%2C...%2Cw_N)，可以推导得到对每个词的平均交叉熵为：

![[公式]](https://www.zhihu.com/equation?tex=H%28W%29%3D-%5Cfrac%7B1%7D%7BN%7D%5Ctext%7Blog%7DP%28w_1%2Cw_2%2C...%2Cw_N%29)

显然，交叉熵越小，则建模的概率分布越接近真实分布。交叉熵描述的是样本的平均编码长度，虽然物理意义很明确，但是不够直观。因此，在此基础上，我们定义**困惑度**(perplexity) ![[公式]](https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%7D+%5Ctext%7BPreplexity%7D%28W%29%26%3D2%5E%7BH%28W%29%7D%5C%5C+%26%3D%5Csqrt%5BN%5D%7B%5Cfrac%7B1%7D%7BP%28w_1%2C...%2Cw_N%29%7D%7D+%5Cend%7Balign%7D)

困惑度在语言模型中的物理意义可以描述为对于任意给定序列，下一个候选词的可选范围大小。同样的，困惑度越小，说明所建模的语言模型越精确。

谷歌找了下资料，发现英文数据集PTB上最新的困惑度结果可以达到47.69，细思极恐，详见[论文](https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1711.03953.pdf)。中文目前还没找到最新的结果，但可以肯定的是基于神经网络的语言模型在困惑度指标上要明显优于传统n-gram语言模型。

## 5. 总结

从特性上可以将 n-gram 语言模型看作是基于词与词**共现频次**的统计，而神经网络语言模型则是给每个词分别赋予分布式向量表征，探索它们在**高维连续空间**中的依赖关系。实验证明，神经网络的分布式表征以及非线性映射更加适合对自然语言进行建模。

最后，稍微提一句语言模型在机器翻译和语音识别中的应用。基于**贝叶斯定理**，这两类应用都可以建模为如下形式

![[公式]](https://www.zhihu.com/equation?tex=P%28y%7Cx%29%3D%5Cfrac%7BP%28x%7Cy%29P%28y%29%7D%7BP%28x%29%7D)

从贝叶斯概率的角度出发，可以将语言模型 ![[公式]](https://www.zhihu.com/equation?tex=P%28y%29) 看作为一种**先验**知识，从而指导最终结果选择。
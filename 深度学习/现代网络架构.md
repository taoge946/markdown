# 现代网络架构


### 一般网络架构的劣势

- 模型想取得更高的正确率，一种显然的思路就是给模型添加更多的层。

- 随着层数的增加，模型的准确率得到提升，然后过拟合;这时再增加更多的层，准确率会开始下降。

- 在到达一定深度后加入更多层，模型可能产生梯度消失（层数过多会使梯度衰减到0）或爆炸（梯度无限大）问题。导致模型不可预测

### 解决办法：

- 可以通过更好的初始化权重、添加BN层等解决

- 使用现代架构，试图通过引入不同的技术来解决这些问题，如残差连接。

## Resnet

### resnet介绍

>  可以把它理解为一个模块，这个模块经过堆叠可以构成一个很深的网络。
>
> ResNet通过增加残差连接显式地让网络中的层拟合残差映射(residual mapping)。

#### Residual Block



![残差网络结构](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210827203613070.png)



> ResNet不再尝试学习x到H(x)的潜在映射，而是学习两者之间的不同，或说残差(residual) 。
> 然后，为了计算H(x)，可将残差加到输入上。假设残差是F(x)= H(x) - x，我们将尝试学习F(x)+ X,
> 而不是直接学习H(x)。

![image-20210827204026107](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210827204026107.png)

> 中间层可以根据需求进行改变，不同就是将权重层与输入进行add操作

#### resnet与其他网络的区别

- 每个ResNet 块都包含一系列层，列残差连接把块的输入加到块的输出上。
  由于加操作是在元素级别执行的，所以输入和输出的大小要一致。如果它们的大小不同，我们可以采用填充或进行1*1的卷积进行改变的方式使得输入与输出的形状相同。
- ResNet网络结构为多个Residual Block的串联
  实验表明学习残差比直接学习输入输出间映射要容易收敛，可达到更高的分类精度，ResNet在**上百层**都有很好的表现。

![image-20210827204710224](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210827204710224.png)



####  resnet网络的结构特点

1. 与纯层的堆叠相比，ResNet多了很多“残差链接”，即shortcut路径，也就是Residual Block;

2. ==ResNet中，所有的Residual Block都没有pooling层，降采样是通过conv的stride实现的==*(要想缩小两倍的话就调 stride为2)*


3. ==通过Average Pooling得到最终的特征，而不是通过全连接层==;
4. ==每个卷积层之后都紧接着BatchNorm层，使得我们可以更快的使用更深的模型==



> ResNet结构非常容易修改和扩展，通过调整block内的channel数量以及堆叠的block数量，就可以很容易地调整网络的宽度和深度来得到不同表达能力的网络,而不用过多地担心网络的“退化”问题，只要训练数据足够，逐步加深网络，就可以获得更好的性能表现。

#### resnet的网络实现

可以看出，一般都是两次卷积再与输出相加，resnet模型中每次卷积都要跟着一个批标准化BN层

![image-20211004112422503](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211004112422503.png)

根据这个特点创造一个resnet的子模型，可以将多个子模型叠加起来构成一个大模型（必须都继承至nn.Moudle)

我们要保证输入和输出的大小一致，这样才能相加，所以卷积时要定义padding，不填充的话图像的长和宽都会变小（卷积核大小-1）个像素。

```python
class resnet_base(nn.Module):
    def __init__(self,in_channel,out_channel):
        super().__init__()
        self.conv1=nn.Conv2d(in_channel,out_channel,kernel_size=3,padding=1,bias=False)#使输入图像和输出图像大小相同，这样才能相加
        self.bn1=nn.BatchNorm2d(out_channel,)#批标准化后这一批的数据就失去了原有的量纲了，就不需要卷积中的b了，就可以不适用b来加速计算
        
        self.conv2=nn.Conv2d(out_channel,out_channel,kernel_size=3,padding=1,bias=False)#使输入图像和输出图像大小相同，这样才能相加
        self.bn2=nn.BatchNorm2d(out_channel,)#批标准化后这一批的数据就失去了原有的量纲了，就不需要卷积中的b了，就可以不适用b来加速计算
    def forward(self,x):
        res=x   #保存输入（残差）以便之后相加
        out=self.conv1(x)
        out=F.relu(self.bn1(out),inplace=False)#这样就直接在原始数据上进行改变，就不保存原始数据了（原本是将值赋到relu中再将结果赋回去，现在直接就地改变），会加快运算速度
        out=self.conv1(x)
        out=F.relu(self.bn1(out),inplace=False)
        out+=res    #输出与输入相加
        return (F.relu(out))
```

可以使用双问号来打开源码如`model??`

![image-20211005220605754](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211005220605754.png)

resnet是一种提取网络特征的一种非常好的架构

- 特点：ResNet结构非常容易修改和扩展，通过调整block内的channe数量以及堆叠的block数量，就可以很容易地调整网络的宽度和深度，来得到不同表达能力的网络，而不用过多地担心网络的“退化”问题，只要训练数据足够，逐步加深网络，就可以获得更好的性能表现。



## Densenet

ResNet使用了残差连接来搭建更深的网络。DenseNet更进一步，它引入了每层与所有后续层的连接，即每一层都接收所有前置层的特征平面作为输入。

![image-20211005221044382](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211005221044382.png)

![image-20211005221126261](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211005221126261.png)

就是说每一层会接收到前面的所有层的输出(特征)

> 此结构主要有以下改进:对比于ResNet的Residual Block，创新性地提出DenseBlock，在每一个Dense Block中，任何两层之间都有直接的连接，也就是说，网络每一层的输入都是前面所有层输出的并集.(concat维度连接,并不是直接相加)
>
> 该层所学习的特征图也会被直接传给其后面所有层作为输入。
> 通过密集连接，缓解梯度消失问题，加强特征传播，鼓励特征复用，极大的减少了参数量。

并不会增加网络的参数量和计算量。
==DenseNet 比其他网络效率更高,其关键就在于网络每层计算量的减少以及特征的重复利用。==
DenseNet则是让I层的输入直接影响到之后的所有层,

![image-20211005221737655](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211005221737655.png)

> 但是,DenseNet的密集连接方式==需要所有的特征图大小保持一致==。
>
> 为了解决这个问题,
> DenseNet网络中使用DenseBlock+Transition的结构,

在DenseNet模型中不能使用pool或是stride的,否则会改变图像大小导致不能相合并

- 其中DenseBlock是包含很多层的模块，每个层的特征图大小相同，层与层之间采用密集连接方式
- 而Transition模块是连接两个相邻的DenseBlock，并且通过Pooling使特征图大小降低。

![image-20211005222129241](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211005222129241.png)

每两个DenseBlock之间通过Transition相当于池化来缩放特征,最后得到输出



![image-20211005222328141](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211005222328141.png)

我们使用这个网络进行鸟类的分类，使用加利福利亚大学的鸟类数据集，一共200多种鸟类，每一类都有几十张。

这个分类问题极具挑战性，因为要分的类别非常的多（200多种），而且每一类的数量都比较少，即使我们人去观察都很难记住两百种鸟

预训练模型的卷积部分可以看成一个特征提取网络

***可以使用卷积部分将特征提取出来，使用这些特征来代表这些图片，这样就不用每次都进行卷积了，每次只需要比对特征值就可以了。***

#### 代码实现

```python
images_path=glob.glob(r'D:/数据集/birds/*/*.jpg')
images_path[100].split('\\')[1].split('.')[1]#使用这种方法将类别名称提取出来

image_name=[p.split("\\")[1].split(".")[1] for p in images_path ]

#对字符串形式的类别编码成数字
unique_labels=np.unique(image_name)#获取所有不相同的类

label_to_index=dict((v,k) for k,v in enumerate(unique_labels))
index_to_label=dict((c,d) for d,c in label_to_index.items())

all_labels=[label_to_index.get(name) for name in image_name]
```

然后就是乱序，切成测试数据和训啦数据

```python
index=np.random.permutation(len(all_labels))
images_path=np.array(images_path)[index]
all_labels=np.array(all_labels)[index]
i=int(len(all_labels*0.8))
train_img=images_path[:i]
train_label=all_labels[:i]
test_img=images_path[i:]
test_label=all_labels[i:]
```

接着就是创建变形方法和dataset子类,需要注意的是如果数据库中有一些图像是灰度图像则需要我们人为的给它添加一个维度并使那个维度的值为3

```python
class birds(data.Dataset):
    def __init__(self,images_path,label):
        super().__init__()
        self.labels=label
        self.imgs=images_path
    def __getitem__(self, index) :
       img=self.imgs[index]
       labels=self.labels[index]
       pil_img=Image.open(img) 
       #若图片是灰度图片，就需要人为的给他添加维度
       #彩色图片的shape有3个，为H,W,C。灰度图片只有H,W,shape是2
       np_img=np.asarray(pil_img,dtype=np.uint8)#先转换成numpy格式的方便使用
       if len(np_img.shape)==2:
           img_data=np.repeat(np_img[:,:,np.newaxis],3,axis=2)#添加一个值为1的新的维度，将最后一个维度重复三次
           pil_img=Image.fromarray(img_data)#再转换成pil_image形式
       img_tensor=transforms(pil_img)
       return img_tensor,labels
    def __len__(self):
        return len(self.imgs)

```

然后就是创建ds，dl集并取出一个批次的数据进行观察

要注意的是我们的目的是进行特征的提取，所以没有必要对训练数据进行乱序

```python
train_ds=birds(train_img,train_label)
test_ds=birds(test_img,test_label)
train_dl=data.DataLoader(train_ds,batch_size=8)#我们只是提取特征，所以没有必要做乱序
test_dl=data.DataLoader(test_ds,batch_size=8)

img_batch,label_batch=next(iter(train_dl))

plt.figure(figsize=(12,8))
for i,(img,name)in enumerate(zip(img_batch,label_batch)):
    plt.subplot(2,4,i+1)
    plt.imshow(img.permute(1,2,0).numpy())
    plt.title(index_to_label[name.item()])#或是index_to_label.get(name.item())
```

![image-20211006162549455](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211006162549455.png)

#### 使用DenseNet来提取特征

先将这个模型下下来观察一下模型结构

![image-20211007104012251](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007104012251.png)

或得模型的卷积基

`model=torchvision.models.densenet121(pretrained=True).features*#这样获得模型的卷积基部分*`

然后将模型放在显卡上，使其变成不可训练

```python
model=model.cuda()
for p in model.parameters():
    p.requires_grad=False
```

然后就是利用DenseNet的卷积基对所有的图片进行特征的提取

```python
train_features=[]
train_fea_labels=[]#创建一个用来储存图像特征的空列表
for im ,la in train_dl:
    out=model(im.cuda())   #这样返回值就是特征,但此时的输出是4维的（batch_size,c,w,h）
                           #模型放到显卡上后，图像数据也要放到显卡上
    out=out.view(out.size(0),-1)#所以要将输出扁平化，第一个维度是batch_size，按批数进行展开
    train_features.extend(out.cpu().data)#将这些扁平化的特征放到列表中，因为可能要在cpu上进行处理所以给cpu，我们只需要它的值，不需要梯度之类的信息，所以.data
    train_fea_labels.extend(la)#标签不做处理

test_features=[]
test_fea_labels=[]
for im ,la in test_dl:
    out=model(im.cuda())   
    out=out.view(out.size(0),-1)
    test_features.extend(out.cpu().data)
    test_fea_labels.extend(la)
```

这样，就将所有的特征通过预训练好的卷积基提取到了两个列表中了，下面我们就可以根据这些特征和标签进行训练，创建一个分类模型了。

```PYTHON
train_features[0].shape#这就是一张图片的特征，有50176个数

class Feature_ds(nn.Module):
    def __init__(self,fea_list,lab_list):
        super().__init__()
        self.fea_list=fea_list
        self.label_list=lab_list
    def __getitem__(self,index):
        return self.fea_list[index],self.label_list[index]
    def __len__(self):
        return len(self.fea_list)
    
train_fea_ds=Feature_ds(train_features,train_fea_labels)
test_fea_ds=Feature_ds(test_features,test_fea_labels)
train_dl=data.DataLoader(train_fea_ds,batch_size=8,shuffle=True)
test_dl=data.DataLoader(test_fea_ds,batch_size=8)
```

然后就可以创建一个全连接的分类模型就可以了，我们这里只定义了一个线性层，也可以再加几层

```python
in_feature_size=train_features[0].shape[0]#torch.size里没有item方法，所以直接.shape[0]将数值提取出来就行

class FCmodel(nn.Module):
    def __init__(self,in_size,out_size):
        super().__init__()
        self.lin1=torch.nn.Linear(in_size,out_size)
    def forward(self,x):
        return self.lin1(x)
    
net=FCmodel(in_feature_size,200)#200分类
```

然后就可以定义损失函数和优化函数再进行训练了,需要注意的是，在训练fit函数中需要将标签y的类型进行更改

```pytho
y=torch.tensor(y,dtype=torch.long)#将标签转化为torch.long 类型
```

最后的结果发现训练集的正确率非常高但测试集只有60多，这是因为

1. 要分类的种类太多了，有200种
2. 每一种的数量太少了，就几十张

注意的点：

- 如果使用数据增强的话，就不能用特征提取了
- 这样先提取特征再根据特征进行分类是训练的非常快的，不需要对图片预处理，图片加载等一系列操作



接下来对图片进行预测时，需要使用卷积基提取特征，再将提取的特征放入模型种获取种类



## Inception

之前讲的所有的现代网络结构都没有考虑不同的卷积核尺寸的影响，inception正是从这个角度出发，用了不同尺度的卷积核要么用1x1,3x3,5x5,7x7的卷积层，要么用平面池化层

它考虑到==不同尺寸的卷积核可能对特征的提取能力不同==，inception会将不同尺寸的卷积组合在一起并联合了所有的输出，使模型的提取能力得到进一步的提升

![image-20211007201623346](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007201623346.png)

它有以下改进：

- 一层block就包含1x1,3x3,5x5卷积和3x3池化，网络每一层都能学习到稀疏或不稀疏（1x1）的特征，既增加了网络的宽度，也增加了网络对尺度的适应性
- 使用concat在每个block后合成特征（维度相加），获得非线性属性

为了降低算力成本，作者在3x3和5x5卷积层之前添加额外的1x1卷积层，来限制输入信道的数量



#### 1*1卷积核

>  1x1的卷积比5x5的卷积廉价很多，而且输入信道数减少也有利于降低算力成本
>
> 不过要注意1x1卷积是在最大池化之后，而不是之前

1x1的卷积核最重要的特点就是将联合

比如对于一组5(c)* 5(h) * 5(w)的输入数据,如果对其使用1*1的卷积核进行卷积，就是将每一个像素点的5个维度（c）联合起来，比如1 *1的卷积核设置为两个输出则这组图像就会变成2 * 5 * 5

1*1卷积核能够缩减channel，==将所有的不同维度联合起来==

![image-20211007204007436](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007204007436.png)

卷积越进行图像越小，channel越多，我们可以***在最后使用1* *  *1的卷积将channel联合起来来代替全连接层***从未保留了空间结构，减少计算参数。

如对一个256*256的输入图像，进行无数次卷积后图像变成了128 * 2*2,这时使用1 * 1卷积来将维度合并，缩减到1*2 *2四个点



我们发现池化层也会对图像的特征提取有一定的作用，所以加入了3*3 最大池化再进行1 *1卷积（记住先池化再1 *1卷积）



#### inception网络结构代码实现

可以看到，这里面用到了非常多的卷积，所以我们先定义一个基础的卷积模型：卷积+BN+激活

```python
class base_con(nn.Module):
    def __init__(self,in_channel,out_channel,**kwargs):#有很多卷积核参数我们不写了用**kwargs表示
        super().__init__()
        self.conv=nn.Conv2d(in_channel,out_channel,bias=False,**kwargs)#因为后面要使用BN层，所以偏置不起作用，所以不如不使用
        self.bn=nn.BatchNorm2d(out_channel)
    def forward(self,x):
        x=self.conv(x)
        x=self.bn(x)
        return F.relu(x,inplace=True)#只要不适用残差就可以直接在原数据上进行更改
```

用上面所定义的卷积模型创建一个inception块

```python
class incepetion_block(nn.Module):
    def __init__(self,in_channel,pool_features):#pool_features是在最后的1x1卷积这里要输出多少个特征
        super().__init__()
        self.b_1x1=base_con(in_channel,64,kernel_size=1)#因为之前输入的参数有**kwargs，所以我们可以直接定义这个函数的其他参数
                                                        #这是模型最左边的1x1的卷积核
        self.b_3x3_1=base_con(in_channel,64,kernel_size=1)
        self.b_3x3_2=base_con(64,96,kernel_size=3,padding=1)#因为卷积核大小为3，所以填充1使得图像大小不变
                                                            #这是左边第二条通道，先1*1再3*3
        self.b_5x5_1=base_con(in_channel,48,kernel_size=1)
        self.b_5x5_2=base_con(48,64,kernel_size=5,padding=2)#这是第三个模块

        self.b_pool=base_con(in_channel,pool_features,kernel_size=1)#因为池化不需要初始化，所以只初始化一个1x1的卷积就行
    def forward(self,x):
        b_1x1_out=self.b_1x1(x)

        b_3x3=self.b_3x3_1(x)
        b_3x3_out=self.b_3x3_2(b_3x3)

        b_5x5=self.b_5x5_1(x)
        b_5x5_out=self.b_5x5_2(b_5x5)

        b_pool_out=F.max_pool2d(x,kernel_size=3,stride=1,padding=1) #也是进行1的填充使得图像大小不变
        b_pool_out=self.b_pool(b_pool_out)

        outputs=[b_1x1_out,b_3x3_out,b_5x5_out,b_pool_out]
        return torch.cat(outputs,dim=1)#沿着第二个维度进行合并，第一个维度时batch_size,第二个是channel
        

```

这个inception块就创建完毕了，可以使用了

#### inception结构应用

在进行分类问题或是图像识别目标检测时，我们可以使用1x1的卷积来代替Linear层

比如我们想进行一个25分类，我们就可以让它最后输出一个1 * 5 *5直接输出结果，用它来表示25分类



比较著名的应用就是GoogleNet，它最重要的特点就是使用了大量的inception block模块来进行模型的构建，它对特征的提取是非常有效的。

现在比较新的inception网络为inceptionNet V3

![image-20211007222604193](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007222604193.png)

![![image-20211007222852060](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007222852060.png)(C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007222628879.png)

![image-20211007222808557](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007222808557.png)

![image-20211007222935820](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007222935820.png)

![image-20211007223030002](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007223030002.png)

![image-20211007223041032](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20211007223041032.png)

在pytorch中内置了inception V3这个模型

`model=torchvision.models.inception_v3(pretrained=True)`


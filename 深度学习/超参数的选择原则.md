# 超参数的选择原则

- 网络容量：与网络中可训练参数（变量个数）成正比，网络中可训练的参数越多，网络容量就越大。

网络中的神经元越多，层数越多，神经网络的拟合能力越强，但会导致训练的速度以及难度增大，更容易产生过拟合现象。

- 超参数：所谓超参数，也就是搭建神经网络中，需要我们自己如选择(不是通过梯度下降算法去优化)的那些参数比如，中间层的神经元个数、学习速率。

### 如何提高网络的拟合能力

一种显然的想法是增大网络容量:

1. 增加层 
2. 增加隐藏神经元个数

单纯的增加神经元的个数对于整个网络的性能提高并不明显，但增加层会大大提高网络的拟合能力，这也是为什么现在深度学习的层越来越深的原因。

单层的神经元个数以及大小不能太小，否则会造成信息瓶颈，使得模型欠拟合。



## 参数选择原则：

理想的模型是刚好在欠拟合和过拟合的界线上，也就是正好拟合数据。

首先开发一个过拟合的模型:

1. 添加更多的层。
2. 让每一层变得更大。
3. 训练更多的轮次

开发一个过拟合的模型保证模型能够足够的学习训练数据

然后，抑制过拟合:

1. dropout
2. 正则化
3. 图像增强

然后再次调节超参数：

1. 学习速率
2. 隐藏单元层
3. 训练轮次

> 超参数的选择是一个经验与不断测试的结果。经典机器学习的方法，如特征工程、增加训练数据也要做交叉验证

***总的原则***是:保证神经网络容量足够拟合数据

一、增大网络容量，直到过拟合

二、采取措施抑制过拟合

三、继续增大网络容量，直到过拟合
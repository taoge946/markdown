# 图像定位与分割

## 定义

- 分类：给定计算机一个图像，计算机能利用预测模型分辨出图像中是什么对象，比如是猫还是狗

  分类问题是最基础最核心的应用，以下都是以分类为基础进行的。

- 分类+定位：不仅知道图像中的对象是什么，还要再对象附近画一个边框确定该对象所处的位置。

- 语义分割：区分到图中每一点像素点，对图像中的所有像素点进行分类，而不仅仅是用矩形框框起来。

- 目标检测：回答图片中有什么在什么位置

- 实例分割：是目标检测和语义分割的结合，相对于目标检测使用边界框将目标框起来，实例分割利用语义分割可以精确到物体的边缘，相对于语义分割，实例分割需要标记处图上同一物体的不同个体（分别标出5只羊）。

  

![image-20210918103320145](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210918103320145.png)

![image-20210918103728166](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210918103728166.png)

## 图像定位：

对于单纯的分类问题，就是给定一张图片，我们输出一个标签。

而定位需要输出4个数字（x,y,w,h）.其中x,y是目标图像的中心,w,h是目标的宽度和高度。有了这四个参数就可以容易的找到物体的边框。

![image-20210919161858455](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210919161858455.png)

前边基础仍是是普通的卷积神经网络来提取网络特征，将得到的特征向量扁平化之后再连接一个全连接层。

- 第一部分是一个简单的分类问题，使用softmax层进行激活，最后来判断图像中是猫的概率是多少，

- 第二部分连接一个全连接层来输出四个数字（x,y,w,h）代表，猫所在的位置，我们使用L2 Loss来进行优化，度量其损失

我们可以看出，这个过程是监督学习，我们需要事先在数据集中标注出图像中物种的类别以及位置，然后就可以训练我们的神经网络让其输出4个值。

> 使用的数据集是Oxford-IIIT数据集，是一个宠物图像数据集，包含37中宠物，每种宠物200张左右的宠物图片，该数据集同时包含宠物分类，头部轮廓标注以及语义分割信息。以下为数据集内容。我们的目的就是给定一张图片，可以标注出头部的位置以及进行语义分割。

![image-20210919164518369](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210919164518369.png)

### 编写代码实现图像定位

使用的数据集有两个文件，第一部分是图片，还有一个是目标值，使用xml格式的文件储存标签信息

（每一张图片中的宠物类别以及头部所在的位置）。

![image-20210920084510077](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210920084510077.png)

可以使用lxml这个python库来对标签信息进行解析。

1. 引入所需的库

```python
from lxml import etree #etree是一个非常好的网页解析模块，速度是最快的，也非常的简单
  					   #填写一个路径进去，就能提取出页面中指定的信息。
from matplotlib.patches import Rectangle#用来绘制正方形（把头在图像上按正方形画出来）
import glob
from PIL import Image#帮我们读取图像

import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn #损失函数以及模型的层都在这个里面
import torch.nn.functional as F
from torch.utils import data
import torchvision
from torchvision import transforms

import os
```

2. 读入图片和标签并转到期望格式

```python
pil_img=Image.open(r'D:/数据集/宠物数据集/dataset/images/Abyssinian_1.jpg')#先读入图片
np_image=np.array(pil_img)#并将其转化为ndarray的形式
xml=open(r'D:\数据集\宠物数据集\dataset\annotations\xmls\Abyssinian_1.xml','r').read()# 读取第一个图片的xml文件
```

3. 根据xml文件获取其中的具体标签值

   使用的网页解析工具`etree` 的`xpath`

```python
sel=etree.HTML(xml) #使用etree来获取信息。想创建一个html格式的选择器。

width=int(sel.xpath('//size/width/text()')[0])#//代表从根目录开始查，一级一级的查到width，text（）代表要获得文本信息，[0]代表只取数值
height=int(sel.xpath('//size/height/text()')[0])#以下同理获取到想要的信息。
xmin=int(sel.xpath('//bndbox/xmin/text()')[0])#转成整形方便后面绘图
xmax=int(sel.xpath('//bndbox/xmax/text()')[0])
ymin=int(sel.xpath('//bndbox/ymin/text()')[0])
ymax=int(sel.xpath('//bndbox/ymax/text()')[0])
```

为了可以更清楚的了解标注信息，可以将标签所标的点绘图绘出。

```python
plt.imshow(np_image)
rect=Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),fill=False,color='red')#画矩形的三个参数为(xmin,ymin),宽度以及高度,不需要填充
ax=plt.gca()#因为在当前坐标系上绘制，所以先获取当前坐标系
ax.axes.add_patch(rect)#在当前坐标系下添加矩形框
```

效果如下

<img src="C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210920211050602.png" alt="image-20210920211050602" style="zoom:100%;" />



再对数据集进行观察，发现数据集中的图像大小都不一致，所以在将其输入进模型之前要先resize到统一大小，但统一图片大小后，图像中的所有坐标值都会发生变化。

所以我们可以将其从数值化为相对于长宽的比值，这样就不管图像怎么变化都可以正常使用。

```python
xmin=(xmin/width)*224
ymin=(ymin/height)*224
xmax=(xmax/width)*224
ymax=(ymax/height)*224
```

4. 创建数据的输入

```python
images=glob.glob(r'D:/数据集/宠物数据集/dataset/images/*.jpg')#获取所有图像目录
anno=glob.glob(r'D:\数据集\宠物数据集\dataset\annotations\xmls\*.xml')#获取所有标签目录
```

发现图片有7千多张但标签只有3千多个，说明该数据集并没有对所有的图片进行标注，所以我们要通过现有的标签筛名称选出所有被标注的图片。

![image-20210920215950958](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210920215950958.png)

可以看出，如果只要名字的话可以将地址进行分割，然后取最后一项再去掉后缀 `.xml`

```python
xml_name=[x.split('\\')[-1].replace('.xml','') for x in anno]#首先对地址通过 '//’ 拆开取最后一项再将后缀替换为空，
															 #对标签中的所有地址进行这个操作
    
#对于文件名也进行如上操作，如果图像名称在已标注的列表中则保留，否则就去掉
imgs=[x for x in images if x.split('\\')[-1].replace('.jpg','') in xml_name]#此时所获取的图片内容都在被标记的标签当中
```

可以看出剩下的图片和标签是一一对应的，然后就可以对anno标签进行处理，提取出所有图片的最小值最大值等特征。

创建一个函数提取出指定xml文件中的所有标签值

```python
def to_labels(path):#定义一个函数来进行xml中的标签的提取
    xml=open(r'{}'.format(path)).read() #这么做防止输入的字符串转义
    sel=etree.HTML(path)
    width=int(sel.xpath('//size/width/text()')[0])
    height=int(sel.xpath('//size/height/text()')[0])
    xmin=int(sel.xpath('//bndbox/xmin/text()')[0])
    xmax=int(sel.xpath('//bndbox/xmax/text()')[0])
    ymin=int(sel.xpath('//bndbox/ymin/text()')[0])
    ymax=int(sel.xpath('//bndbox/ymax/text()')[0])
    return [xmin/width,ymin/height,xmax/width,ymax/height]#用列表来返回值，每一个标签返回一组数
```

使用：

```python
labels=[to_labels(p) for p in anno]
```

![image-20210921093556128](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210921093556128.png)

下面就需要来分割train数据和text数据

```python
i=int(len(imgs)*0.8)

train_imgs=imgs[:i]
train_labels=labels[:i]
text_imgs=imgs[i:]
text_labels=labels[i:]

#最好还是分成四个再reshape成1维的，否则输入模型时会因为不是1维的而报错
out1_label, out2_label, out3_label, out4_label = list(zip(*labels))

out1_label = np.array(out1_label).astype(np.float32).reshape(-1, 1)[index]
out2_label = np.array(out2_label).astype(np.float32).reshape(-1, 1)[index]
out3_label = np.array(out3_label).astype(np.float32).reshape(-1, 1)[index]
out4_label = np.array(out4_label).astype(np.float32).reshape(-1, 1)[index]

out1_train_label = out1_label[:i]
out2_train_label = out2_label[:i]
out3_train_label = out3_label[:i]
out4_train_label = out4_label[:i]

test_images = images[i: ]
out1_test_label = out1_label[i: ]
out2_test_label = out2_label[i: ]
out3_test_label = out3_label[i: ]
out4_test_label = out4_label[i: ]

```

创建变形方法以及ds子类以及dl

```python
transform=transforms.Compose([
    transforms.Resize((224,224)),#不要少括号，有两个括号一个时函数的一个是大小，不要少逗号
    transforms.ToTensor()
])

class Mydataset(data.Dataset):
    def __init__(self,img_path,labels_list):
        self.imgs=img_path
        self.labels=labels_list
    def __getitem__(self,index):
        img=self.imgs[index]
        pil_img=Image.open(img) 
        img_tensor=transform(pil_img) #可以直接对一张图片进行变形 
        l1,l2,l3,l4=self.labels[index] #切出来后是一个长度为4的列表,所以可以直接解剖成4个值
        return img_tensor,l1,l2,l3,l4
    def __len__(self):
        return len(self.imgs)

train_ds=Mydataset(train_imgs,train_labels)
text_ds=Mydataset(text_imgs,text_labels)
train_dl=data.DataLoader(train_ds,batch_size=batch_size,shuffle=True)
text_dl=data.DataLoader(text_ds,batch_size=batch_size)

#改成分别对四个坐标值进行操作，因为图片可能格式不同，所以加一个判定
class Oxford_dataset(data.Dataset):
    def __init__(self, img_paths, out1_label, out2_label, 
                       out3_label, out4_label, transform):
        self.imgs = img_paths
        self.out1_label = out1_label
        self.out2_label = out2_label
        self.out3_label = out3_label
        self.out4_label = out4_label
        self.transforms = transform
        
    def __getitem__(self, index):
        img = self.imgs[index]
        out1_label = self.out1_label[index]
        out2_label = self.out2_label[index]
        out3_label = self.out3_label[index]
        out4_label = self.out4_label[index]
        pil_img = Image.open(img) 
        imgs_data = np.asarray(pil_img, dtype=np.uint8)
        if len(imgs_data.shape) == 2:
            imgs_data = np.repeat(imgs_data[:, :, np.newaxis], 3, axis=2)
            img_tensor = self.transforms(Image.fromarray(imgs_data))
        else:
            img_tensor = self.transforms(pil_img)
        return (img_tensor, 
                out1_label, 
                out2_label, 
                out3_label, 
                out4_label)
    
    def __len__(self):
        return len(self.imgs)

train_ds = Oxford_dataset(train_images, out1_train_label, 
                               out2_train_label, out3_train_label,
                               out4_train_label, transform)
test_ds = Oxford_dataset(test_images, out1_test_label, 
                               out2_test_label, out3_test_label,
                               out4_test_label, transform)
train_dl=data.DataLoader(train_ds,batch_size=8,shuffle=True)
text_dl=data.DataLoader(text_ds,batch_size=8)

```

取一个批次的数据进行可视化来看是否符合预期

```python
(img_batch,out1,out2,out3,out4)=next(iter(train_dl)) #取一个切片的数据来进行可视化看是否符合预期

plt.figure(figsize=(12,8))
for i,(img,l1,l2,l3,l4) in enumerate(zip(img_batch[:2],out1[:2],out2[:2],out3[:2],out4[:2])):
    img=img.permute(1,2,0).numpy()
    plt.subplot(1,2,i+1)#创建子画布，一共一行两列，每次在第i+1张子画布上进行绘图。
    plt.imshow(img)
    xmin,ymin,xmax,ymax=l1*224,l2*224,l3*224,l4*224 #注意to_labels()函数阿几个比值的返回顺序，要与其一致
    rect=Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),fill=False,color='red')
    ax=plt.gca()
    ax.axes.add_patch(rect)
```

![image-20210921215037211](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210921215037211.png)

可以看出正确的对头部进行了绘制。接下来就是创建定位模型了。本次使用卷积基resnet101来提取模型的特征，作为模型的一部分.

与迁移学习不同，我们只选择resnet的预训练模型作为初始值而不是将其其他层冻结，所以我们可以将其层取出来

***提取预训练模型的卷积基***

```python
resnet=torchvision.models.resnet101(pretrained=True)

list(resnet.children()) #会返回一个所有层的生成器，用list将其返回,可以从下图看出，一共有十块
list(resnet.children())[:-1] #可以像这样取出除了最后一个模块linear层之外的所有层，将其添加到我们的模型当中

conv_base=nn.Sequential(*list(resnet.children())[:-1])#将除了线性层之外的所有层放入我们的卷积基模型中，使用‘*’将其解包
```

![image-20210922165224217](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210922165224217.png)

然后使用卷积基来创建自己的模型

```python
in_size=resnet.fc.in_features#通过这个获取resnet全连接层的输入参数量

class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()#继承父类的方法
        self.conv_base=nn.Sequential((*list(resnet.children())[:-1]))#初始化卷积部分，使用之前迁移学习的卷积基
        self.fc1=nn.Linear(in_size,1)#
        self.fc2=nn.Linear(in_size,1)
        self.fc3=nn.Linear(in_size,1)
        self.fc4=nn.Linear(in_size,1)#因为输出了四个坐标值（xmin,ymin,xmax,ymax）所以设置了4个linear层
    def forward(self,x):
        x=self.conv_base(x)
        x = x.view(x.size(0), -1)
        x1=self.fc1(x)
        x2=self.fc1(x)
        x3=self.fc1(x)
        x4=self.fc1(x)
        return x1,x2,x3,x4

```

然后就是定义损失以及优化还有fit函数并将模型放在gpu上

```python
if torch.cuda.is_available():
    model.to('cuda')

loss_fn=nn.MSELoss()#因为这是一个回归问题，返回的是一个坐标值，一个具体的数，所以定义的lossfn为这个均方误差

from torch.optim import lr_scheduler
optim=torch.optim.Adam(model.parameters(),lr=0.0001)
exp_lr_scheduler=lr_scheduler.StepLR(optim,step_size=7,gamma=0.1)

```



```python
#注意缩进要对齐
#因为是回归问题，只是得到四个坐标，没有正确率一说，所以只看loss不看正确率
def fit(epoch,model,trainloader,textloader):   
    running_loss=0
    model.train()      #使用dropout层和BN层时一定要加这一句表明模型正处于训练状态下  
    for x,y1,y2,y3,y4 in trainloader:  
        x,y1,y2,y3,y4=(x.to('cuda'),y1.to('cuda'),y2.to('cuda'),y3.to('cuda'),y4.to('cuda')) #如果要使用GPU进行加速计算就打开屏蔽 
        y_pred1,y_pred2,y_pred3,y_pred4,=model(x)
        loss1=loss_fn(y_pred1,y1)
        loss2=loss_fn(y_pred2,y2)
        loss3=loss_fn(y_pred3,y3)
        loss4=loss_fn(y_pred4,y4)
        loss=loss1+loss2+loss3+loss4
        optim.zero_grad()
        loss.backward()
        optim.step()
        with torch.no_grad():                  
            running_loss+=loss.item()          #每一批样本的loss
        
    exp_lr_scheduler.step() #如果定义了学习速率衰减可以打开这个屏蔽
    
    epoch_loss=running_loss/len(trainloader.dataset)  #平均loss为一个批次的总loss除以训练的长度
    
                                
    text_running_loss=0
    model.eval()      #告诉模型现在是预测模式，dropout层不需要发挥作用  
    with torch.no_grad():                  
            for x,y1,y2,y3,y4  in textloader: 
                x,y1,y2,y3,y4=(x.to('cuda'),y1.to('cuda'),y2.to('cuda'),y3.to('cuda'),y4.to('cuda'))
                y_pred1,y_pred2,y_pred3,y_pred4=model(x)
                loss1=loss_fn(y_pred1,y1)
                loss2=loss_fn(y_pred2,y2)
                loss3=loss_fn(y_pred3,y3)
                loss4=loss_fn(y_pred4,y4)
                loss=loss1+loss2+loss3+loss4                    
                text_running_loss+=loss.item()
                    
    epoch_test_loss=running_loss/len(textloader.dataset)  
        
    
    print('epoch: ', epoch, 
        'loss： ', round(epoch_loss, 3),
        'test_loss： ', round(epoch_test_loss, 3),
            )
        
    return epoch_loss, epoch_test_loss

epochs=5
train_loss=[]
test_loss=[]

for epoch in range(epochs):
    epoch_loss, epoch_test_loss = fit(epoch, model, train_dl, text_dl)
    train_loss.append(epoch_loss)
    test_loss.append(epoch_test_loss)
```

可以看出loss非常小，可以直接用。

![image-20210926113718035](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210926113718035.png)

将loss的变化绘图

```python
plt.figure()
plt.plot(range(1,len(train_loss)+1),train_loss,'r',label='Training_loss')
plt.plot(range(1,len(test_loss)+1),test_loss,'bo',label='Valuinging_loss')
plt.title('loss')
plt.xlabel('epoch')
plt.ylabel('loss')
plt.legend()
plt.show()
```

![image-20210926113802703](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210926113802703.png)

接下来保存模型权重

```pythoin
model_save_path='pet_head_model.pth'
torch.save(model.state_dict(),model_save_path)
```

然后就可以对这个模型进行预测了，==预测时所输入的数据必须满足创建的dl的规格==，是以dl为单位输入的，要是网上找的图片必须打成dataloader包再输入，或是直接取出test_dl的一个切片进行观察

```python
new_model=Net()
new_model.load_state_dict(torch.load(model_save_path))
new_model.to('cuda')#导入训练好的模型并放到显卡上

plt.figure(figsize=(12,8))#建一个大点的画布
imgs,_,_,_,_=next(iter(text_dl))#”_“是占位符，只是占一个位置，我们不需要dl中的4个坐标值，我们自己预测，我们只取出图片
if torch.cuda.is_available:
    imgs=imgs.to('cuda')#将这张图片放到显卡中
out1,out2,out3,out4=new_model(imgs)#将这张图片放入模型中会返回四个坐标值
for i in range(6):
    plt.subplot(2,3,i+1)#创建一个六行一列的画布
    plt.imshow(imgs[i].permute(1,2,0).cpu().numpy())
    xmin, ymin, xmax, ymax = out1[i].item()*224, out2[i].item()*224, out3[i].item()*224, out4[i].item()*224
    rect = Rectangle((xmin, ymin), (xmax-xmin), (ymax-ymin), fill=False, color='red')
    ax = plt.gca()
    ax.axes.add_patch(rect)
```

![image-20210926124420104](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210926124420104.png)

可以看出效果相对理想

## 宠物图像分类与定位

我们可以再根据数据集的名称对定位的宠物图片进行宠物分类，识别出每张图片都分别是什么种类的宠物

获取数据集的方法如上，分别获得所有图像和标签地址，取出所有标签文件的名称并将有标签的图像留下,再定义一个获取标签文件内具体标签值的函数来获取具体的标签值

```python
images=glob.glob(r'D:/数据集/宠物数据集/dataset/images/*.jpg')#获取所有图像目录
anno=glob.glob(r'D:\数据集\宠物数据集\dataset\annotations\xmls\*.xml')#获取所有标签目录

xml_name=[x.split('\\')[-1].replace('.xml','') for x in anno]#首先对地址通过 // 拆开取最后一项再将后缀替换为空，对标签中的所有地址进行这个操作
imgs=[x for x in images if x.split('\\')[-1].replace('.jpg','') in xml_name]

def to_labels(path):#定义一个函数来进行xml中的标签的提取
    xml=open(r'{}'.format(path)).read() #这么做防止输入的字符串转义
    sel=etree.HTML(xml)
    width=int(sel.xpath('//size/width/text()')[0])
    height=int(sel.xpath('//size/height/text()')[0])
    xmin=int(sel.xpath('//bndbox/xmin/text()')[0])
    xmax=int(sel.xpath('//bndbox/xmax/text()')[0])
    ymin=int(sel.xpath('//bndbox/ymin/text()')[0])
    ymax=int(sel.xpath('//bndbox/ymax/text()')[0])
    return [xmin/width,ymin/height,xmax/width,ymax/height] #这样返回的是一组值的数，每一个标签返回一组数

labels=[to_labels(p) for p in anno]
```

然后再把种类给加上，根据标签将所有种类提取出来（不重复），再按顺序留一份标签名称以便创建ds用

```python
species=[]
name=[]
for i in xml_name:
    a=i.split('_')[0]#观察得知每个数据的名字都是教 ***_10 前面是名字后面是数字构成，所以用_ 分割后取第一个
    name.append(a)
    if (a not in species):#列表的一个应用
         species.append(a)

species_to_idx=dict((v,k) for k,v in enumerate(species))#根据种类找到序号            
idx_to_speic=dict((c,j) for j,c in species_to_idx.items())#根据序号找到种类

for i in range(len(labels)):
    labels[i].append(species_to_idx[name[i]])#按顺序留一份标签名称以便创建ds
    
```

==以上没有考虑到数据集的名称命名格式，是错误的，这个数据集的命名格式有的宠物是多个单词，他们之间也是用‘_’向连接所以如果直接用' _'来切的话会切多，所以进行了改进，切掉最后一个数字之前的再加空格利用字符串的加法加起来==

```python
spe=[]
for i in species:
    a=''
    for j in i:
        a+=' '
        a+=j
    spe.append(a)
names=[]
for i in name:
    a=''
    for j in i:
        a+=' '
        a+=j
    names.append(a)

species_to_idx=dict((v,k) for k,v in enumerate(spe))#根据种类找到序号
idx_to_speic=dict((c,j) for j,c in species_to_idx.items())#根据序号找到种类

for i in range(len(labels)):
    labels[i].append(species_to_idx[names[i]])#按顺序留一份标签名称以便创建ds
```

这样就可以显示有多个单词的宠物种类了

<img src="C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210929131738951.png" alt="image-20210929131738951" style="zoom:150%;" />

然后就是和上面类似的创建ds，dl集了

```python
l1,l2,l3,l4,name=list(zip(*labels))
index=np.random.permutation(len(imgs))    
images=np.array(imgs)[index]  

out1 = np.array(l1).astype(np.float32).reshape(-1, 1)[index]
out2 = np.array(l2).astype(np.float32).reshape(-1, 1)[index]
out3 = np.array(l3).astype(np.float32).reshape(-1, 1)[index]
out4 = np.array(l4).astype(np.float32).reshape(-1, 1)[index]
name_1=np.array(name).astype(np.float32).reshape(-1, 1)[index]

i=int(len(imgs)*0.8)

train_imgs=images[:i]
l1_train=out1[:i]
l2_train=out2[:i]
l3_train=out3[:i]
l4_train=out4[:i]
name_train=name_1[:i]

text_imgs=images[i:]
l1_text=out1[i:]
l2_text=out2[i:]
l3_text=out3[i:]
l4_text=out4[i:]
name_text=name_1[i:]

transform=transforms.Compose([
    transforms.Resize((224,224)),
    transforms.ToTensor()
])

class Oxford_dataset(data.Dataset):
    def __init__(self, img_paths, out1_label, out2_label, 
                       out3_label, out4_label,name_label, transform):
        self.imgs = img_paths
        self.out1_label = out1_label
        self.out2_label = out2_label
        self.out3_label = out3_label
        self.out4_label = out4_label
        self.name_label = name_label
        self.transforms = transform
        
    def __getitem__(self, index):
        img = self.imgs[index]
        out1_label = self.out1_label[index]
        out2_label = self.out2_label[index]
        out3_label = self.out3_label[index]
        out4_label = self.out4_label[index]
        name_label = self.name_label[index]
        pil_img = Image.open(img) 
        imgs_data = np.asarray(pil_img, dtype=np.uint8)
        if len(imgs_data.shape) == 2:
            imgs_data = np.repeat(imgs_data[:, :, np.newaxis], 3, axis=2)
            img_tensor = self.transforms(Image.fromarray(imgs_data))
        else:
            img_tensor = self.transforms(pil_img)
        return (img_tensor, 
                out1_label, 
                out2_label, 
                out3_label, 
                out4_label,
                name_label)
    
    def __len__(self):
        return len(self.imgs)

train_ds=Oxford_dataset(train_imgs,l1_train,l2_train,l3_train,l4_train,name_train,transform)
text_ds=Oxford_dataset(text_imgs,l1_text,l2_text,l3_text,l4_text,name_text,transform)
train_dl=data.DataLoader(train_ds,batch_size=4,shuffle=True)
text_dl=data.DataLoader(text_ds,batch_size=4)
```

再取一张看看

`(img_batch,out1,out2,out3,out4,name2)=next(iter(train_dl))`

模型的最后一个线性层要改成37，因为这样有37类

```python
class Net(nn.Module):
    def __init__(self):
        super(Net,self).__init__()
        self.conv_base=nn.Sequential(*list(resnet.children())[:-1])
        self.fc1=nn.Linear(in_size,1)
        self.fc2=nn.Linear(in_size,1)
        self.fc3=nn.Linear(in_size,1)
        self.fc4=nn.Linear(in_size,1)
        self.fc5=nn.Linear(in_size,37)#改成37
    def forward(self,x):
        x=self.conv_base(x)
        x = x.view(x.size(0), -1)
        x1=self.fc1(x)
        x2=self.fc2(x)
        x3=self.fc3(x)
        x4=self.fc4(x)
        name=self.fc5(x)
        return x1,x2,x3,x4,name
```

用到了两个损失，图像分类用交叉熵损失函数，4个点的坐标用均分误差损失，将所有损失相加将损失和反向传播

```python
loss_fn=nn.MSELoss()#因为这是一个回归问题，返回的是一个坐标值，一个具体的数，所以定义的lossfn为这个均方误差
loss_fn2=nn.CrossEntropyLoss()


def fit(epoch,model,trainloader,textloader):   #因为这个函数没有输入train_x和train_y所以直接用train_dl和text_dl来算正确率
    running_loss=0
    correct=0                            #通过记录预测正确的个数和样本总数之间的比值来看正确率
    total=0
    model.train()      #使用dropout层和BN层时一定要加这一句表明模型正处于训练状态下  
    for x,y1,y2,y3,y4,name in trainloader:  
        x,y1,y2,y3,y4,name=(x.to('cuda'),y1.to('cuda'),y2.to('cuda'),y3.to('cuda'),y4.to('cuda'),name.to('cuda')) #如果要使用GPU进行加速计算就打开屏蔽 
        y_pred1,y_pred2,y_pred3,y_pred4,name_pred=model(x)
        loss1=loss_fn(y_pred1,y1)
        loss2=loss_fn(y_pred2,y2)
        loss3=loss_fn(y_pred3,y3)
        loss4=loss_fn(y_pred4,y4)
        name_loss=loss_fn2(name_pred,name.squeeze().long())#要压缩维度成1维的要不会报错
        loss=loss1+loss2+loss3+loss4+name_loss
        optim.zero_grad()
        loss.backward()
        #name_loss.backward()
        optim.step()
        with torch.no_grad():                  
            running_loss+=loss.item()          #每一批样本的loss
            name_pred=torch.argmax(name_pred,dim=1)
            correct+=(name_pred==name).sum().item()  #每一次正确的个数加到correct里
            total+=name.size(0)
        
    exp_lr_scheduler.step() #如果定义了学习速率衰减可以打开这个屏蔽
    epoch_acc=correct/total 
    epoch_loss=running_loss/len(trainloader.dataset)  #平均loss为一个批次的总loss除以训练的长度
    
    text_correct=0                          #这是对测试数据 ,对于测试数据不用反向传播不用优化
    text_total=0                             
    text_running_loss=0
    model.eval()      #告诉模型现在是预测模式，dropout层不需要发挥作用  
    with torch.no_grad():                  
            for x,y1,y2,y3,y4 ,name in textloader: 
                x,y1,y2,y3,y4,name=(x.to('cuda'),y1.to('cuda'),y2.to('cuda'),y3.to('cuda'),y4.to('cuda'),name.to('cuda'))
                y_pred1,y_pred2,y_pred3,y_pred4,name_pred=model(x)
                loss1=loss_fn(y_pred1,y1)
                loss2=loss_fn(y_pred2,y2)
                loss3=loss_fn(y_pred3,y3)
                loss4=loss_fn(y_pred4,y4)
                name_loss=loss_fn2(name_pred,name.squeeze().long())
                loss=loss1+loss2+loss3+loss4+name_loss                    
                text_running_loss+=loss.item()
                name_pred=torch.argmax(name_pred,dim=1)
                text_correct+=(name_pred==name).sum().item()  #每一次正确的个数加到correct里
                text_total+=name.size(0)
                    
    epoch_test_loss=running_loss/len(textloader.dataset)  
    epoch_text_acc=text_correct/text_total      
    
    print('epoch: ', epoch, 
        'accuracy: ',round(epoch_acc,3),
        'loss： ', round(epoch_loss, 3),
        'test_loss： ', round(epoch_test_loss, 3),
        'text_accuracy: ',round(epoch_text_acc,3)
            )
        
    return epoch_loss, epoch_acc,epoch_test_loss,epoch_text_acc
```





训练结果如下：

![image-20210929134408403](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210929134408403.png)



![image-20210929122347820](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210929122347820.png)

![image-20210929134549804](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210929134549804.png)

结果图，勉勉强强。






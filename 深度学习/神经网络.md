## 神经网络

- 神经元：装有一个0~1之间数字的容器。比如说图像中每一个像素都是一个神经元，代表该点像素的灰度值。

  这个数字叫做激活值，激活值越大，这个神经元越亮

一张图片的这些神经组成了神经网络的第一层，最后一层是全连接层负责输出结果，最后一层的激活值代表着是其的可能性。

- 网络中间还有几层==隐含层==，可以看成是一个大黑箱

![image-20210905213242646](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210905213242646.png)

- 在网络运行时，上一层的激活值将决定下一层的激活值，所以说神经网络处理信息的核心机制是一层的激活值是通过怎样的运算算出下一层的激活值的
- ~~在理想情况下，（前向传播）倒数第二层中的各个神经元能分别对应上一个笔画部件，当我们输入某一个值的时候，其对应的部件的激活值会趋近于一，最后一层就看不同的部件能组合出什么样的数字了，神经网络就是像这样将不同的神经元一层一层的进行组合~~
  - ~~每个神经元（像素）都有其固定的位置，根据其权重不同就可以在下一层网络中组合出不同的形状，再一层层进行组合~~
  - 只有卷积神经网络才会获得具体的特征，普通的神经网络并不能获取到具体的特征
- ![image-20210905213950832](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210905213950832.png)

### 对于单个神经元

- 上一层的激活值乘以权重相加后可能不在0~1的范围内，所以可以用激活函数如sigmoid将其缩入0 ~1

- 若经过一层的加权后有的神经元的激活值是负的，这些负的神经元会拖累整个集合的和（正常的激活值是0~1），说明该神经元不该激活，所以将所有负数的神经元的激活值全部变成0不关注它们。

- 所以下一层的某一个神经元的激活值就相当于这一组的所有权值有多正确进行打分

- 有时即使加权和大于0了也不想将其点亮，我们可以加==偏置b==，保证其不能随便激发，过早激发可能会出现偏差

  如大于10才激活，就可以加一个-10的偏置

  ![image-20210905215452649](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210905215452649.png)

- 权重告诉我们第二层神经元关注什么样的像素图案，偏置告诉我们加权和得有多大才能让神经元的激发变得有意义

### 对于多个神经元

这一层的每一个神经元都会和上一层的所有神经元相连接，每一个链接上都有一个权重，下一层的每个神经元都在加权后有一个偏置，目的就是找到正确的权重以及偏置

![](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210905220518067.png)

​                                                                                                      ~矩阵表示神经网络的运作~

机器学习和线性代数是不分家的

- 神经元其实是一个输出为0~1函数，整个神经网络也是一个函数，输入图像的像素值，输出分类概率
- 现在sigmoid已经过时了（很难训练），使用relu激活函数可以进行更好训练（RELU(a)=max(0,a)）小于0不激发大于0激发

### 梯度下降法

###### 1维

- 权重可以看成是连接的强弱，偏置表明神经元是否更容易被激活 
- 在一开始，我们会完全随机的初始化所有的权重和偏置值，建立一个检测用的函数（==代价函数==或叫损失函数）告诉计算机什么时候错了，这个代价函数的输入是所有的权重和偏置，输出表明这些权重和偏置有多差劲；我们还要告诉计算机如何去优化
- 优化方法（代价函数的梯度下降）：随机在函数上找到一个点，让其向左或向右移动，看其斜率的变化再慢慢改变步长最终逼近函数的某个局部最小值点。因为是随机选的点，所以会得到所有的局部最小值的点

  - 其中，如果每步的大小和斜率成比例，在最小值附近斜率会越来越平缓，每步会越来越小，这样可以防止调过头
- ![image-20210907194947958](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907194947958.png)
- ![image-20210907194925991](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907194925991.png)

##### 2维

输入可以想象成一个XY平面，代价函数是平面上方的曲面，此时就要求偏导，求梯度了。沿梯度的负方向走下降最快

![image-20210907195426122](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907195426122.png)

与1维所不同的就是我们需要计算梯度

升级到n维，我们同样也是计算该点的梯度然后根据梯度来修改权重以及偏置，这意味着输入训练集的每一份样本的输出都会使结果更靠近正确结果。

- 需要注意的是，这个代价函数取了整个训练集的平均值
- 梯度下降也可以理解为改变哪些权重所造成的影响力最大

这种按照负梯度的倍数不停的调整函数输入值的过程就叫**梯度下降法**

对梯度进行缩放的参数被称为**学习速率(learning rate)**

> 计算一个样本的代价需要求出网络的输出与期待的输出之间每一项的差的平方和，然后这一轮中所有的样本的代价都求出来后最后取平均，就得到了整个网络的代价值![image-20210907202544937](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907202544937.png)
>
> 我们所要求的是代价函数的负梯度，它会告诉你如何改变所有连线上的权重偏置才好让代价下降的最快

> 梯度向量每一项的大小是告诉我们代价函数对每个参数有多敏感，我们不只看数值的下降，还看哪个参数的性价比最高<img src="C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907203550697.png" alt="image-20210907203550697" />

#### 反向传播

神经网络学习的核心算法

一个还没有训练好的神经网络最后的输出结果可能是五花八门的

<img src="C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907203845305.png" alt="image-20210907203845305" style="zoom:50%;" />

​	我们希望最终的分类结果是2，就要提高2这个神经元的激活值（到1）并降低其他的激活值（到0），并且变动的大小和目标值之间的差成正比

​	**为了增加这个激活值，我们可以**

1. 增加偏置：

2. 增加权重: 各个权重的影响力不同，上一层的激活值越大该权重的影响力就越大，所以增大激活值大的神经元的权重就更有用
   <img src="C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907204702119.png" alt="image-20210907204702119" style="zoom:80%;" />

3. 改变上一层的激活值:

   ​	使所有正权重连接的神经元更亮（更大），使所有负权重连接的神经元更暗，我们可以根据权重的大小，对激活值做出呈比例的改变，但我们不能直接改变激活值，所以我们只能去改前一层的权重和偏置

---



​       这只是对2这个神经元的增强所需要上一层所进行的改变，还要考虑那些需要削弱的神经元对上一层的要求，将所有输出神经元的期待全部加起来最为最终的如何改变上一层神经元的指示

![image-20210907205603312](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907205603312.png)

<img src="C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907205642445.png" alt="image-20210907205642445" style="zoom:50%;" />

然后我们重复这个过程直到第一层，来使得所有的权重都能识别出2。这只是一个样本训练了一遍，我们需要将所有的样本训练很多遍直至能够识别出所有的手写数字

![image-20210907213440141](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907213440141.png)

然后求取一个平均值当作这一轮中最终的权重调整



![image-20210907213758056](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907213758056.png)

### 一些加快训练的技巧

​	但是这样一个一个的将所有训练集进行训练的话还是太慢了，所以

- batch的概念就被提出来了，一次训练一个bitch，计算一个batch来作为梯度下降的一步来大幅提高训练速度

- 随机梯度下降：随机进行大步的滑动代替之前走一步算一步（右图）

  ![image-20210907215133974](C:\Users\tao'ge\AppData\Roaming\Typora\typora-user-images\image-20210907215133974.png)



## 卷积神经网络

### 卷积核

​	一般以随机小数矩阵的形式初始化，在网络的训练过程中卷积核将学习得到合理的权值。共享权值(卷积核)带来的直接好处是减少网络各层之间的连接，同时又降低了过拟合的风险。子采样也叫做池化(pooling)，通常有均值子采样(mean pooling)和最大值子采样(max pooling)两种形式。子采样可以看作一种特殊的卷积过程。卷积和子采样大大简化了模型复杂度，减少了模型的参数。

### 卷积通道数以及参数数量

​	每一层卷积有多少channel数，以及一共有多少层卷积，这些暂时没有理论支撑，一般都是靠感觉去设置几组候选值，然后通过实验挑选出其中的最佳值。这也是现在深度卷积神经网络虽然效果拔群，但是一直为人诟病的原因之一。

​	多说几句，每一层卷积的channel数和网络的总卷积层数，构成了一个巨大的超参集合，这个超参集合里的最优组合，很可能比目前业界各种fancy的结构还要高效。只是我们一来没有理论去求解这个超参集合里的最优，二来没有足够的计算资源去穷举这个超参集合里的每一个组合，因此我们不知道这个超参集合里的最优组合是啥。

![img](file:///C:\Users\tao'ge\AppData\Local\Temp\ksohtml24000\wps1.png)
	四个通道上每个通道对应一个2*2的卷积核，这4个2*2的卷积核上的参数是不一样的，之所以说它是1个卷积核，是因为把它看成了一个4*2*2的卷积核，4代表一开始卷积的通道数，2*2是卷积核的尺寸，实际卷积的时候其实就是4个2*2的卷积核（这四个2*2的卷积核的参数是不同的）分别去卷积对应的4个通道，然后相加，再加上偏置b，注意b对于这四通道而言是共享的，所以b的个数是和最终的featuremap的个数相同的，先将w2忽略，只看w1，那么在通道的某位置（i,j）处的值，是由四个通道上（i,j）处的卷积结果相加，再加上偏置b1，然后再取激活函数值得到的。  所以最后得到两个feature map， 即输出层的卷积核个数为 feature map 的个数。也就是说卷积核的个数=最终的featuremap的个数，卷积核的大小=开始进行卷积的通道数*每个通道上进行卷积的二维卷积核的尺寸（此处就是4*（2*2）），b（偏置)的个数=卷积核的个数=featuremap的个数。

 

参数的数目为4×（2×2）×2+2个

 

​	在实际应用中，往往使用多层卷积，然后再使用全连接层进行训练，多层卷积的目的是一层卷积学到的特征往往是局部的，层数越高，学到的特征就越全局化。

 

### Forward**前向传播**

 前向传播就是从输入到输出

​	前向过程的卷积为典型valid的卷积过程，即卷积核kernalW覆盖在输入图inputX上，对应位置求积再求和得到一个值并赋给输出图OutputY对应的位置。每次卷积核在inputX上移动一个位置，从上到下从左到右交叠覆盖一遍之后得到输出矩阵outputY(如图4.1与图4.3所示)。如果卷积核的输入图inputX为Mx*Nx大小，卷积核为Mw*Nw大小，那么输出图Y为(Mx-Mw+1)*(Nx-Nw+1)大小。

![img](file:///C:\Users\tao'ge\AppData\Local\Temp\ksohtml24000\wps2.png) 

 

​	前向传播就是一个矩阵相乘的过程，在进行预测的时候就是使用已经确定的参数进行前向传播

 

### BackForward**反向传播**

 反向传播是从输出到输入

​	在错误信号反向传播过程中，先按照神经网络的错误反传方式得到尾部分类器中各神经元的错误信号，然后错误信号由分类器向前面的特征抽取器传播。错误信号从子采样层的特征图(subFeatureMap)往前面卷积层的特征图(featureMap)传播要通过一次full卷积过程来完成。这里的卷积和上一节卷积的略有区别。如果卷积核kernalW的长度为Mw*Mw的方阵，那么subFeatureMap的错误信号矩阵Q_err需要上下左右各拓展Mw-1行或列，与此同时卷积核自身旋转180度。subFeatureMap的错误信号矩阵P_err等于featureMap的误差矩阵Q_err卷积旋转180度的卷积核W_rot180。

 

下图错误信号矩阵Q_err中的A，它的产生是P中左上2*2小方块导致的，该2*2的小方块的对A的责任正好可以用卷积核W表示，错误信号A通过卷积核将错误信号加权传递到与错误信号量为A的神经元所相连的神经元a、b、d、e中，所以在下图中的P_err左上角的2*2位置错误值包含A、2A、3A、4A。同理，我们可以论证错误信号B、C、D的反向传播过程。综上所述，错误信号反向传播过程可以用下图中的卷积过程表示。

![img](file:///C:\Users\tao'ge\AppData\Local\Temp\ksohtml24000\wps3.png) 

 

***\*权值更新过程中的卷积\****

卷积神经网络中卷积层的权重更新过程本质是卷积核的更新过程。由神经网络的权重修改策略我们知道一条连接权重的更新量为该条连接的前层神经元的兴奋输出乘以后层神经元的输入错误信号，卷积核的更新也是按照这个规律来进行。

![img](file:///C:\Users\tao'ge\AppData\Local\Temp\ksohtml24000\wps4.png) 

在前向卷积过程中，卷积核的每个元素(链接权重)被使用过四次，所以卷积核每个元素的产生四个更新量。把前向卷积过程当做切割小图进行多个神经网络训练过程，我们得到四个4*1的神经网络的前层兴奋输入和后层输入错误信号，如图所示。